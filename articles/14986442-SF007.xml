<?xml version="1.0"?><api><query><pages><page pageid="14986442" ns="0" title="History of statistics"><revisions><rev xml:space="preserve">{{History of science sidebar}}
 
{{See|History of probability}}

The '''history of statistics''' can be said to start around 1749 although, over time, there have been changes to the interpretation of what the word ''[[statistics]]'' means. In early times, the meaning was restricted to information about [[Sovereign state|states]]. This was later extended to include all collections of information of all types, and later still it was extended to include the analysis and interpretation of such data. In modern terms, &quot;statistics&quot; means both sets of collected information, as in [[national accounts]] and [[temperature record]]s, and analytical work which requires [[statistical inference]]. 

Statistical activities are often associated with models expressed using [[probability|probabilities]], and require [[probability theory]] for them to be put on a firm [[Theory (mathematical logic)|theoretical basis]]: see [[History of probability]].

A number of statistical concepts have had an important impact on a wide range of sciences. These include the [[design of experiments]] and approaches to statistical inference such as [[Bayesian inference]], each of which can be considered to have their own sequence in the development of the ideas underlying modern statistics.

==Introduction==
By the 18th century, the term &quot;[[statistics]]&quot; designated the [[official statistics|systematic collection]] of [[demographic]] and [[economics|economic]] data  by states. In the early 19th century, the meaning of &quot;statistics&quot; broadened, then including the discipline concerned with the collection, summary, and analysis of data. Today statistics is widely employed in government, business, and all the sciences. Electronic [[computer]]s have expedited [[computational statistics|statistical computation]], and have allowed statisticians to develop &quot;computer-intensive&quot; methods.

The term &quot;[[mathematical statistics]]&quot; designates the mathematical theories of [[probability theory|probability]] and [[statistical inference]], which are used in [[applied statistics|statistical practice]]. The relation between statistics and probability theory developed rather late, however. In the 19th century, statistics increasingly used [[probability theory]], whose initial results were found in the 17th and 18th centuries, particularly in the analysis of [[games of chance]] (gambling). By 1800, astronomy used probability models and statistical theories, particularly the [[method of least squares]], which was invented by [[Adrien-Marie Legendre|Legendre]] and [[Gauss]]. Early probability theory and statistics was systematized and extended by [[Laplace]]; following Laplace, probability and statistics have been in continual development. In the 19th century, social scientists used statistical reasoning and probability models to advance the new sciences of [[experimental psychology]] and [[sociology]]; physical scientists used statistical reasoning and probability models to advance the new sciences of [[thermodynamics]] and [[statistical mechanics]]. The development of statistical reasoning was closely associated with the development of [[inductive logic]] and the [[scientific method]].

Statistics is not a field of [[mathematics]] but an autonomous [[mathematical science]], like [[computer science]] or [[operations research]]. Unlike mathematics, statistics had its origins in [[public administration]] and maintains a special concern with [[demography]] and [[economics]]. Being concerned with the [[scientific method]] and [[inductive logic]], statistical theory has close association with the [[philosophy of science]]; with its emphasis on learning from data and making best predictions, statistics has great overlap with the [[decision science]] and [[microeconomics]]. With its concerns with [[data]], statistics has overlap with [[information science]] and [[computer science]].

==Etymology==
&lt;!-- {{Wiktionary|statistics}} replaced by line below as leaves huge vertical gap when combined with {{History of science sidebar}} --&gt;
:''Look up '''[[wikt:statistics|statistics]]''' in [[wiktionary]], the free dictionary.''
The term ''statistics'' is ultimately derived from the [[New Latin]] ''statisticum collegium'' (&quot;council of state&quot;) and the [[Italian language|Italian]] word ''statista'' (&quot;[[statesman]]&quot; or &quot;[[politician]]&quot;). The [[German language|German]] ''Statistik'', first introduced by [[Gottfried Achenwall]] (1749), originally designated the analysis of [[data]] about the [[State (polity)|state]], signifying the &quot;science of state&quot; (then called ''political arithmetic'' in English). It acquired the meaning of the collection and classification of data generally in the early 19th century. It was introduced into English in 1791 by [[Sir John Sinclair]] when he published the first of 21 volumes titled ''Statistical Account of Scotland''.&lt;ref&gt;{{cite book|last1 = Ball|first1 = Philip|title = Critical Mass|publisher =  Farrar, Straus and Giroux|year = 2004|isbn=0374530416|page=53 }}&lt;/ref&gt;

Thus, the original principal purpose of ''Statistik'' was data to be used by governmental and (often centralized) administrative bodies. The collection of data about states and localities continues, largely through [[List of national and international statistical services|national and international statistical services]].  In particular, [[censuses]] provide regular information about the [[population]].

The first book to have 'statistics' in its title was &quot;Contributions to Vital Statistics&quot; by
[[Francis GP Neison]], actuary to the [[Medical Invalid and General Life Office]]
(1st ed., 1845; 2nd ed., 1846; 3rd ed., 1857).

== Origins in probability ==
{{See also|Timeline of probability and statistics}}
The earliest writing on statistics was found in a 9th century book entitled: &quot;Manuscript on Deciphering Cryptographic Messages&quot;, written by [[Al-Kindi]] (801â873 AC). In his book, [[Al-Kindi]] gave a detailed description of how to use [[statistics]] and [[frequency analysis]] to decipher encrypted messages, this was the birth of both statistics and cryptanalysis&lt;ref name=sim2000&gt;{{cite book|last=Singh|first=Simon|title=The code book : the science of secrecy from ancient Egypt to quantum cryptography|year=2000|publisher=Anchor Books|location=New York|isbn=0385495323|edition=1st Anchor Books ed.}}&lt;/ref&gt;&lt;ref name=ibr1992&gt;[[Ibrahim A. Al-Kadi]] &quot;The origins of cryptology: The Arab contributionsâ, ''[[Cryptologia]]'', 16(2) (April 1992) pp. 97&amp;ndash;126.&lt;/ref&gt;. 

The &quot;[[Nuova Cronica]]&quot;, a 14th century [[history of Florence]] by the Florentine banker and official [[Giovanni Villani]], includes many statistical information on population, ordinances, commerce and trade, education, and religious facilities and has been described as the first introduction of statistics as a positive element in history,&lt;ref&gt;Villani, Giovanni. EncyclopÃ¦dia Britannica. [[EncyclopÃ¦dia Britannica 2006 Ultimate Reference Suite DVD]]. Retrieved on 2008-03-04.&lt;/ref&gt; though neither the term nor the concept of statistics as a specific field yet existed. But this was proven to be incorrect after the rediscovery of [[Al-Kindi]]'s book on [[frequency analysis]]&lt;ref name=sim2000 /&gt;&lt;ref name=ibr1992 /&gt;.

The mathematical methods of statistics emerged from [[probability theory]], which can be dated to the correspondence of [[Pierre de Fermat]] and [[Blaise Pascal]] (1654). [[Christiaan Huygens]] (1657) gave the earliest known scientific treatment of the subject. [[Jakob Bernoulli]]'s ''[[Ars Conjectandi]]'' (posthumous, 1713) and [[Abraham de Moivre]]'s ''[[The Doctrine of Chances]]'' (1718) treated the subject as a branch of mathematics. See [[Ian Hacking]]'s ''The Emergence of Probability''{{citation needed|date=November 2010}} and [[James Franklin (philosopher)|James Franklin]]'s ''The Science of Conjecture: Evidence and Probability Before Pascal''{{citation needed|date=November 2010}} for histories of the early development of the very concept of mathematical probability. In the modern era, the work of [[Kolmogorov]] has been instrumental in formulating the fundamental model of Probability Theory, which is used throughout statistics.

The [[theory of errors]] may be traced back to [[Roger Cotes]]' ''Opera Miscellanea'' (posthumous, 1722), but a memoir prepared by [[Thomas Simpson]] in 1755 (printed 1756) first applied the theory to the discussion of errors of observation. The reprint (1757) of this memoir lays down the [[axiom]]s that positive and negative errors are equally probable, and that there are certain assignable limits within which all errors may be supposed to fall; continuous errors are discussed and a probability curve is given.

[[Pierre-Simon Laplace]] (1774) made the first attempt to deduce a rule for the combination of observations from the principles of the theory of probabilities. He represented the law of probability of errors by a curve. He deduced a formula for the mean of three observations. He also gave (1781) a formula for the law of facility of error (a term due to [[Joseph Louis Lagrange]], 1774), but one which led to unmanageable equations. [[Daniel Bernoulli]] (1778) introduced the principle of the maximum product of the probabilities of a system of concurrent errors.

The [[method of least squares]], which was used to minimize errors in data [[measurement]], was published independently by [[Adrien-Marie Legendre]] (1805), [[Robert Adrain]] (1808), and [[Carl Friedrich Gauss]] (1809). Gauss had used the method in his famous 1801 prediction of the location of the [[dwarf planet]] [[Ceres (dwarf planet)|Ceres]]. Further proofs were given by Laplace (1810, 1812), Gauss (1823), [[James Ivory (mathematician)|Ivory]] (1825, 1826), Hagen (1837), [[Friedrich Bessel|Bessel]] (1838), &lt;!-- [[W. F. Donkin|Donkin]] --&gt; Donkin (1844, 1856), [[John Herschel|Herschel]] (1850), [[Morgan Crofton|Crofton]] (1870), and [[Thorvald N. Thiele|Thiele]] (1880, 1889).

Other contributors were Ellis (1844), [[Augustus De Morgan|De Morgan]] (1864), [[Glaisher]] (1872), and [[Giovanni Schiaparelli]] (1875). Peters's (1856) formula for &lt;math&gt;r&lt;/math&gt;, the &quot;probable error&quot; of a single observation was widely used and inspired early [[robust statistics]] (resistant to [[Peirce's criterion|outliers]]).

In the 19th century authors on [[statistical theory]] included Laplace, [[Sylvestre Lacroix|S. Lacroix]] (1816), Littrow (1833), [[Richard Dedekind|Dedekind]] (1860), Helmert (1872), [[Hermann Laurent|Laurant]] (1873), Liagre, Didion, [[Augustus De Morgan|De Morgan]], [[George Boole|Boole]], [[Francis Ysidro Edgeworth|Edgeworth]], and [[Karl Pearson|K. Pearson]].

[[Adolphe Quetelet]] (1796â1874), another important founder of statistics, introduced the notion of the &quot;average man&quot; (''l'homme moyen'') as a means of understanding complex social phenomena such as [[crime rates]], [[marriage rates]], or [[suicide rates]].

==Design of experiments==
In 1747, while serving as surgeon on HM Bark ''Salisbury'', [[James Lind (physician)|James Lind]] carried out a controlled experiment to develop a cure for [[scurvy]].&lt;ref name=&quot;ADC1997&quot;&gt;{{Cite journal| last =Dunn | first =Peter | coauthors = | title =James Lind (1716-94) of Edinburgh and the treatment of scurvy  | journal =Archive of Disease in Childhood Foetal Neonatal | volume =76 | issue = 1| pages =64â65 | publisher =British Medical Journal Publishing Group | location =United Kingdom |date = January 1997| url =http://fn.bmj.com/cgi/content/full/76/1/F64 | doi = 10.1136/fn.76.1.F64| pmc =1720613 | accessdate =2009-01-17 | pmid=9059193}}&lt;/ref&gt; In this study his subjects' cases &quot;were as similar as I could have them&quot;, that is he provided strict entry requirements to reduce extraneous variation. The men were paired, which provided [[Blocking (statistics)|blocking]]. From a modern perspective, the main thing that is missing is randomized allocation of subjects to treatments.

A theory of statistical inference was developed by [[Charles Sanders Peirce|Charles S. Peirce]] in &quot;[[Charles Sanders Peirce bibliography#illus|Illustrations of the Logic of Science]]&quot; (1877â1878) and &quot;[[Charles Sanders Peirce bibliography#SIL|A Theory of Probable Inference]]&quot; (1883), two publications that emphasized the importance of randomization-based inference in statistics.

In another study, Peirce randomly assigned volunteers to a [[blinding (medicine)|blinded]], [[repeated measures design|repeated-measures design]] to evaluate their ability to discriminate weights.&lt;ref name=&quot;smalldiff&quot;&gt;{{Cite news| author=[[Charles Sanders Peirce]] and [[Joseph Jastrow]]|year=1885|title=On Small Differences in Sensation|url=http://psychclassics.yorku.ca/Peirce/small-diffs.htm| journal=Memoirs of the National Academy of Sciences|volume=3|pages=73â83}}&lt;/ref&gt;&lt;ref name=&quot;telepathy&quot;&gt;{{Cite news|first=Ian |last=Hacking|  authorlink=Ian Hacking | title=Telepathy: Origins of Randomization in Experimental Design|journal=[[Isis (journal)|Isis]]|issue=A Special Issue on Artifact and Experiment|volume=79|number=3| month=September|year=1988 |pages=427â451|jstor=234674 | mr = 1013489}}&lt;/ref&gt;&lt;ref name=&quot;stigler&quot;&gt;{{Cite news|author=[[Stephen M. Stigler]]|title=A Historical View of Statistical Concepts in Psychology and Educational Research| journal=American Journal of Education| volume=101|number=1|month=November|year=1992|pages=60â70}}&lt;/ref&gt;&lt;ref name=&quot;dehue&quot;&gt;{{Cite news|author=Trudy Dehue|title=Deception, Efficiency, and Random Groups: Psychology and the Gradual Origination of the Random Group Design|journal=[[Isis (journal)|Isis]]|volume=88|number=4|month=December|year=1997|pages=653â673}}&lt;/ref&gt;
Peirce's experiment inspired other researchers in psychology and education, which developed a research tradition of randomized experiments in laboratories and specialized textbooks in the 1800s.&lt;ref name=&quot;smalldiff&quot;/&gt;&lt;ref name=&quot;telepathy&quot;/&gt;&lt;ref name=&quot;stigler&quot;/&gt;&lt;ref name=&quot;dehue&quot;/&gt;

[[Charles Sanders Peirce|Charles S. Peirce]] also contributed the first English-language publication on an [[optimal design]] for [[Regression analysis|regression]]-[[statistical model|models]] in 1876.&lt;ref&gt;{{Cite news| author=[[Charles Sanders Peirce|Peirce, C. S.]] | year=1876| title=Note on the Theory of the Economy of Research | journal=Coast Survey Report | pages=197â201}}, actually published 1879, NOAA [http://docs.lib.noaa.gov/rescue/cgs/001_pdf/CSC-0025.PDF#page=222 PDF Eprint].&lt;br /&gt; Reprinted in ''[[Charles Sanders Peirce bibliography#CP|Collected Papers]]'' '''7''', paragraphs 139â157, also in ''[[Charles Sanders Peirce bibliography#W|Writings]]'' '''4''', pp. 72â78, and in {{Cite news| author=[[Charles Sanders Peirce|Peirce, C.&amp;nbsp;S.]] | year=1967
| title=Note on the Theory of the Economy of Research
| journal=[http://or.journal.informs.org/cgi/content/abstract/15/4/643 ''Operations Research'']
|volume=15 | number=4 |month=JulyâAugust|pages=643â648
| jstor=168276|doi=10.1287/opre.15.4.643
}}&lt;/ref&gt; A pioneering [[optimal design]] for [[polynomial regression]] was suggested by [[Joseph Diaz Gergonne|Gergonne]] in 1815. In 1918 Kirstine Smith published optimal designs for polynomials of degree six (and less).

The use of a sequence of experiments, where the design of each may depend on the results of previous experiments, including the possible decision to stop experimenting, was pioneered&lt;ref&gt;Johnson, N.L. (1961). &quot;Sequential analysis: a survey.&quot; ''[[Journal of the Royal Statistical Society]]'', Series A. Vol. 124 (3), 372&amp;ndash;411. (pages 375&amp;ndash;376)&lt;/ref&gt; by [[Abraham Wald]] in the context of sequential tests of statistical hypotheses.&lt;ref&gt;Wald, A. (1945) &quot;Sequential Tests of Statistical Hypotheses&quot;, [[Annals of Mathematical Statistics]], 16 (2), 117&amp;ndash;186.&lt;/ref&gt; [[Herman Chernoff]] wrote an overview of optimal sequential designs,&lt;ref name=&quot;ref3&quot;&gt;[[Herman Chernoff|Chernoff, H.]] (1972) ''Sequential Analysis and Optimal Design'', [[SIAM]] Monograph&lt;/ref&gt; while [[Minimisation (clinical trials)|adaptive designs]] have been surveyed by S. Zacks.&lt;ref&gt;Zacks, S. (1996) &quot;Adaptive Designs for Parametric Models&quot;. In: Ghosh, S. and Rao, C. R., (Eds) (1996). &quot;Design and Analysis of Experiments,&quot; ''Handbook of Statistics'', Volume 13. North-Holland. ISBN 0-444-82061-2.  (pages 151&amp;ndash;180)&lt;/ref&gt; One specific type of sequential design is the &quot;two-armed bandit&quot;, generalized to the [[multi-armed bandit]], on which early work was done by [[Herbert Robbins]] in 1952.&lt;ref&gt;{{cite journal | doi = 10.1090/S0002-9904-1952-09620-8 | last1 = Robbins | first1 = H. | year = 1952 | title = Some Aspects of the Sequential Design of Experiments | url = | journal = Bulletin of the American Mathematical Society | volume = 58 | issue = 5| pages = 527â535 }}&lt;/ref&gt;

A methodology for designing experiments was proposed by [[Ronald Fisher|Ronald A. Fisher]], in his innovative book ''[[The Design of Experiments]]'' (1935).  As an example, he described how to test the [[hypothesis]] that a certain lady could distinguish by flavour alone whether the milk or the tea was first placed in the cup. While this sounds like a frivolous application, it allowed him to illustrate the most important ideas of experimental design: see [[Lady tasting tea]].

==Inference==

[[Charles Sanders Peirce|Charles S. Peirce]] (1839â1914) formulated frequentist theories of estimation and hypothesis-testing in (1877â1878) and (1883), in which he introduced &quot;[[confidence interval|confidence]]&quot;. Peirce also introduced [[Blinding|blinded]], [[Randomized controlled trial|controlled randomized experiments]] with a [[repeated measures design]].&lt;ref&gt;{{cite journal|doi=10.1086/354775|first=Ian |last=Hacking|  authorlink=Ian Hacking | title=Telepathy: Origins of Randomization in Experimental Design|journal=[[Isis (journal)|Isis]]|issue=A Special Issue on Artifact and Experiment|volume=79|number=3| month=September|year=1988 |pages=427â451|jstor=234674 | mr = 1013489}}&lt;/ref&gt; Peirce invented an [[optimal design]] for experiments on gravity.

===Bayesian statistics===

[[Image:Pierre-Simon Laplace.jpg|right|thumb|Pierre-Simon, marquis de Laplace, one of the main early developers of Bayesian statistics.]]

The term ''Bayesian'' refers to [[Thomas Bayes]] (1702&amp;ndash;1761), who proved a special case of what is now called [[Bayes' theorem]]. However, it was [[Pierre-Simon Laplace]] (1749&amp;ndash;1827) who introduced a general version of the theorem and used it to approach problems in [[celestial mechanics]], medical statistics, [[Reliability (statistics)|reliability]], and [[jurisprudence]].&lt;ref&gt;Sigler (1990, Chapter 3)&lt;/ref&gt;  When insufficient knowledge was available to specify an informed prior, Laplace used [[uniform distribution|uniform]] priors, according to his &quot;[[principle of insufficient reason]]&quot;.&lt;ref&gt;Hald (1998), Stigler (1990)&lt;/ref&gt; Laplace also introduced primitive versions of [[conjugate prior]]s and the [[Bernsteinâvon Mises theorem|theorem]] of [[Richard von Mises|von Mises]] and [[S. N. Bernstein|Bernstein]], according to which the posteriors corresponding to initially differing priors ultimately agree, as the number of observations increases.&lt;ref&gt;
Lucien Le Cam (1986) ''Asymptotic Methods in Statistical Decision Theory'': Pages 336 and 618&amp;ndash;621 (von Mises and Bernstein).&lt;/ref&gt; This early Bayesian inference, which used uniform priors following Laplace's [[principle of insufficient reason]], was called &quot;[[inverse probability]]&quot; (because it [[Inductive reasoning|infer]]s backwards from observations to parameters, or from effects to causes &lt;ref name=Fienberg2006&gt;Stephen. E. Fienberg, (2006) [http://ba.stat.cmu.edu/journal/2006/vol01/issue01/fienberg.pdf When did Bayesian Inference become &quot;Bayesian&quot;?] ''Bayesian Analysis'', 1 (1), 1&amp;ndash;40. See page 5.&lt;/ref&gt;).

After the 1920s, [[inverse probability]] &lt;!-- (Bayesian inference with uniform priors following Laplace) --&gt; was largely supplanted  by a collection of methods that were developed by [[Ronald A. Fisher]], [[Jerzy Neyman]] and [[Egon Pearson]]. Their methods came to be called [[frequentist statistics]].&lt;ref name=Fienberg2006&gt;Stephen. E. Fienberg, [http://ba.stat.cmu.edu/journal/2006/vol01/issue01/fienberg.pdf When did Bayesian Inference become &quot;Bayesian&quot;?] ''Bayesian Analysis'' (2006).&lt;/ref&gt; Fisher rejected the Bayesian view, writing that &quot;the theory of inverse probability is founded upon an error, and must be wholly rejected&quot; .&lt;ref name=&quot;ba.stat.cmu.edu&quot;&gt;Aldrich, A., [http://ba.stat.cmu.edu/journal/2008/vol03/issue01/aldrich.pdf R. A. Fisher on Bayes and Bayes' Theorem], Bayesian analysis (2008), 3, number 1, pp. 161&amp;ndash;170&lt;/ref&gt; At the end of his life, however, Fisher expressed greater respect for the essay of Bayes, which Fisher believed to have anticipated his own, [[fiducial inference|fiducial]] approach to probability; Fisher still maintained that Laplace's views on probability were &quot;fallacious rubbish&quot;.&lt;ref name=&quot;ba.stat.cmu.edu&quot; /&gt; Neyman started out as a &quot;quasi-Bayesian&quot;, but subsequently developed [[confidence interval]]s (a key method in frequentist statistics) because &quot;the whole theory would look nicer if it were built from the start without reference to Bayesianism and priors&quot;.&lt;ref&gt;{{cite journal | last1 = Neyman | first1 = J. | year = 1977 | title = Frequentist probability and frequentist statistics | url = | journal = Synthese | volume = 36 | issue = 1| pages = 97â131 | doi = 10.1007/BF00485695 }}&lt;/ref&gt;
The word ''Bayesian'' appeared in the 1930s, and by the 1960s it became the term preferred by those dissatisfied with the limitations of frequentist statistics.&lt;ref name=&quot;Fienberg2006&quot; /&gt;&lt;ref name=&quot;Miller Earliest Uses&quot;&gt;Jeff Miller, [http://jeff560.tripod.com/b.html &quot;Earliest Known Uses of Some of the Words of Mathematics (B)&quot;]&lt;/ref&gt;

In the 20th century, the ideas of Laplace were further developed in two different directions, giving rise to ''objective'' and ''subjective'' currents in Bayesian practice. In the objectivist stream, the statistical analysis depends on only the model assumed and the data analysed.&lt;ref name=Bernardo&gt;{{cite journal | doi = 10.1016/S0169-7161(05)25002-2 | last1 = Bernardo | first1 = JM. | authorlink = JosÃ©-Miguel Bernardo | author-separator =, | author-name-separator= | year = 2005 | title = Reference analysis | url = | journal = Handbook of statistics | volume = 25 | issue = | pages = 17â90 }}&lt;/ref&gt; No subjective decisions need to be involved. In contrast, &quot;subjectivist&quot; statisticians deny the possibility of fully objective analysis for the general case.

In the further development of Laplace's ideas, subjective ideas predate objectivist positions. The idea that 'probability' should be interpreted as 'subjective degree of belief in a proposition' was  proposed, for example, by [[John Maynard Keynes]] in the early 1920s. This idea was taken further by [[Bruno de Finetti]] in Italy (''Fondamenti Logici del Ragionamento Probabilistico'', 1930) and [[Frank P. Ramsey|Frank Ramsey]] in Cambridge (''The Foundations of Mathematics'', 1931).&lt;ref&gt;Gillies, D. (2000), ''Philosophical Theories of Probability''. Routledge. ISBN 041518276X  pp&amp;nbsp;50&amp;ndash;1&lt;/ref&gt; The approach was devised to solve problems with the [[frequentist| frequentist definition of probability]] but also with the earlier, objectivist approach of Laplace.&lt;ref name=Bernardo/&gt; The subjective Bayesian methods were further developed and popularized in the 1950s by [[Leonard Jimmie Savage|L.J. Savage]].{{Citation needed|date=September 2010}}

Objective Bayesian inference was further developed due to [[Harold Jeffreys]], whose seminal book &quot;Theory of probability&quot; first appeared in 1939. In 1957, [[Edwin Thompson Jaynes|Edwin Jaynes]] promoted the concept of [[maximum entropy]] for constructing priors, which is an important principle in the formulation of objective methods, mainly for discrete problems. In 1965, [[Dennis Lindley]]'s 2-volume work &quot;Introduction to Probability and Statistics from a Bayesian Viewpoint&quot; brought Bayesian methods to a wide audience. In 1979, [[JosÃ©-Miguel Bernardo]] introduced [[Prior probability#Uninformative priors|reference analysis]],&lt;ref name=Bernardo/&gt; which offers a general applicable framework for objective analysis. Other well-known proponents of Bayesian probability theory include [[I.J. Good]], [[B.O. Koopman]], [[Howard Raiffa]], [[Robert Schlaifer]] and [[Alan Turing]].

&lt;!--The Bayesian viewpoint has an axiomatic basis. In 1946, [[Richard Threlkeld Cox|Richard T. Cox]] showed that the rules of Bayesian inference follow from a set of constraints, including the representation of degrees of belief by real numbers, the need for consistency, and two [[functional equations]] &lt;ref name = &quot;vkdmsn&quot;/&gt;. Another fundamental justification of the Bayesian approach is [[De Finetti's theorem]], which was formulated in 1930.&lt;ref&gt;de Finetti, B. (1930), Funzione caratteristica di un fenomeno aleatorio. Mem. Acad. Naz. Lincei. 4, 86&amp;ndash;133&lt;/ref&gt;
--&gt;
In the 1980s, there was a dramatic growth in research and applications of Bayesian methods, mostly attributed to the discovery of [[Markov chain Monte Carlo]] methods, which removed many of the [[computational problem]]s, and an increasing interest in nonstandard, complex applications.&lt;ref&gt;Wolpert, RL. (2004) A conversation with James O. Berger, Statistical science, 9, 205&amp;ndash;218&lt;/ref&gt; Despite growth of Bayesian research, most undergraduate teaching is still based on frequentist statistics.&lt;ref&gt;[[JosÃ©-Miguel Bernardo|JosÃ© M. Bernardo]] (2006) [http://www.ime.usp.br/~abe/ICOTS7/Proceedings/PDFs/InvitedPapers/3I2_BERN.pdf A Bayesian mathematical statistics primer]. ICOTS-7&lt;/ref&gt; Nonetheless, Bayesian methods are widely accepted and used, such as for example in the field of [[machine learning]].&lt;ref name=&quot;ReferenceA&quot;&gt;Bishop, C.M. (2007) Pattern Recognition and Machine Learning. Springer, 2007&lt;/ref&gt;

== Statistics today ==
During the 20th century, the creation of precise instruments for [[agricultural research]], [[public health]] concerns ([[epidemiology]], [[biostatistics]], etc.), industrial [[quality control]], and economic and social purposes ([[unemployment]] rate, [[econometry]], etc.) necessitated substantial advances in statistical practices.

Today the use of statistics has broadened far beyond its origins. Individuals and organizations use statistics to understand data and make informed decisions throughout the natural and social sciences, medicine, business, and other areas.

Statistics is generally regarded not as a subfield of mathematics but rather as a distinct, albeit allied, field. Many [[university|universities]] maintain separate mathematics and statistics [[academic department|departments]]. Statistics is also taught in departments as diverse as [[psychology]], [[education]], and [[public health]].

== Important contributors to statistics ==
{{See also|List of statisticians|Founders of statistics}}

&lt;div style=&quot;-moz-column-count:4; column-count:4;&quot;&gt;
* [[Thomas Bayes]]
* [[George E. P. Box]]
* [[Pafnuty Chebyshev]]
* [[David R. Cox]]
* [[Gertrude Mary Cox|Gertrude Cox]]
* [[Harald CramÃ©r]]
* [[Francis Ysidro Edgeworth]]
* [[Bradley Efron]]
* [[Bruno de Finetti]]
* [[Ronald A. Fisher]]
* [[Francis Galton]]
* [[Carl Friedrich Gauss]]
* [[William Sealey Gosset]] (âStudentâ)
* [[Andrey Kolmogorov]]
* [[Pierre-Simon Laplace]]
* [[Erich Leo Lehmann|Erich L. Lehmann]]
* [[Aleksandr Lyapunov]]
* [[Abraham De Moivre]]
* [[Jerzy Neyman]]
* [[Florence Nightingale]]
* [[Blaise Pascal]]
* [[Karl Pearson]]
* [[Charles Sanders Peirce|Charles S. Peirce]]
* [[Adolphe Quetelet]]
* [[C. R. Rao]]
* [[Walter A. Shewhart]]
* [[Charles Spearman]]
* [[Thorvald N. Thiele]]
* [[John Tukey]]
* [[Abraham Wald]]
&lt;/div&gt;

== References ==

&lt;references/&gt;

== Bibliography ==
*{{Cite book |title=A History of Probability and Statistics and Their Applications before 1750 |last=Hald |first=Anders |authorlink=Anders Hald |coauthors= |year=2003 |publisher=Wiley |location=Hoboken, NJ |isbn=0471471291 |page= |pages= |url= }}
*{{Cite book |title=A History of Mathematical Statistics from 1750 to 1930 |last=Hald |first=Anders |authorlink=Anders Hald |coauthors= |year=1998 |publisher=Wiley |location=New York |isbn=0471179124 |page= |pages= |url= }}
* Kotz, S., Johnson, N.L. (1992,1992,1997). ''Breakthroughs in Statistics'', Vols I,II,III. Springer ISBN 0-387-94037-5, ISBN 0-387-94039-1, ISBN 0-387-94989-5
*{{Cite book |title=The History of Statistics in the 17th and 18th Centuries against the changing background of intellectual, scientific and religious thought (Lectures by Karl Pearson given at University College London during the academic sessions 1921-1933) |last=Pearson |first=Egon |authorlink=Egon Pearson |coauthors= |year=1978 |publisher=MacMillan Publishng Co., Inc. |location=New York |isbn=0028501209 |page= |pages=744 |url= }}
* [[David Salsburg|Salsburg, David]] (2001). ''[[The Lady Tasting Tea|The Lady Tasting Tea: How Statistics Revolutionized Science in the Twentieth Century]]''. ISBN 0-7167-4106-7
*{{Cite book
 | author = Stigler, Stephen M.
 | authorlink = Stephen Stigler
 | year = 1990
 | title = The History of Statistics: The Measurement of Uncertainty before 1900
 | publisher = Belknap Press/Harvard University Press
 | isbn = 0-674-40341-X
}}
* Stigler, Stephen M. (1999) ''Statistics on the Table: The History of Statistical Concepts and Methods''. Harvard University Press. ISBN 0-674-83601-4
* {{cite jstor|2684625}}

== External links ==
* [http://www.jehps.net/publications.htm JEHPS: Recent publications in the history of probability and statistics]
* [http://www.jehps.net/indexang.html Electronic Journ@l for History of Probability and Statistics/Journ@l Electronique d'Histoire des ProbabilitÃ©s et de la Statistique]
* [http://www.economics.soton.ac.uk/staff/aldrich/Figures.htm Figures from the History of Probability and Statistics (Univ. of Southampton)]
* [http://www.york.ac.uk/depts/maths/histstat Materials for the History of Statistics (Univ. of York)]
* [http://www.economics.soton.ac.uk/staff/aldrich/Probability%20Earliest%20Uses.htm Probability and Statistics on the Earliest Uses Pages (Univ. of Southampton)]
*[http://jeff560.tripod.com/stat.html Earliest Uses of Symbols in Probability and Statistics] on [http://jeff560.tripod.com/mathsym.html Earliest Uses of Various Mathematical Symbols]
*{{Cite document | title = From Association to Causation: Some Remarks on the History of Statistics | author = David Freedman | publisher = [[University of California, Berkeley]] | year = 2002 | url = http://www.stat.berkeley.edu/~census/521.pdf | format = pdf}}

{{Statistics}}

{{DEFAULTSORT:History Of Statistics}}
[[Category:History of statistics| ]]
[[Category:History of science|Statistics]]
[[Category:History of mathematics|Statistics]]
[[Category:Statistics]]

[[ca:HistÃ²ria de l'estadÃ­stica]]
[[eu:Estatistikaren historia]]
[[fr:Histoire de la statistique franÃ§aise]]
[[pt:HistÃ³ria da estatÃ­stica]]</rev></revisions></page></pages></query></api>
