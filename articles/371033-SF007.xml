<?xml version="1.0"?><api><query><pages><page pageid="371033" ns="0" title="Wget"><revisions><rev xml:space="preserve">{{Infobox software
| name                   = Wget
| screenshot             = 
| caption                = Screenshot of Wget in [[GNU/Linux]]
| developer              = Giuseppe Scrivano, Hrvoje NikÅ¡iÄ
| released               = {{Start date and age|1996|January|df=yes}}
| latest release version = 1.13.4
| latest release date    = {{Start date and age|2011|09|17|df=yes}}
| operating system       = [[Cross-platform]]
| programming language   = [[C (programming language)|C]]
| genre                  = [[FTP client]] / [[HTTP client]]
| license                = [[GNU General Public License]] version 3 and later&lt;ref name=&quot;license&quot;&gt;{{cite web |url=http://hg.addictivecode.org/wget/mainline/file/dd55a5b05511/README |title=README file}}&lt;/ref&gt;
| website                = {{URL|http://www.gnu.org/software/wget}}
}}
'''GNU Wget''' (or just '''Wget''', formerly '''Geturl''') is a [[computer program]] that retrieves content from [[web servers]], and is part of the [[GNU Project]]. Its name is derived from ''[[World Wide Web]]'' and [[HTTP GET|''get'']]. It supports downloading via [[HTTP]], [[HTTPS]], and [[File Transfer Protocol|FTP]] protocols.

Its features include recursive download, conversion of links for offline viewing of local HTML, support for proxies, and much more.  It appeared in 1996, coinciding with the boom of popularity of the Web, causing its wide use among [[Unix]] users and distribution with most major [[GNU/Linux]] based distributions.  Written in [[porting|portable]] [[C (programming language)|C]], Wget can be easily installed on any Unix-like system and has been ported to many environments, including [[Microsoft Windows]], [[Mac OS X]], [[OpenVMS]], [[MorphOS]] and [[AmigaOS]].

It has been used as the basis for graphical programs such as [[GWget]] for the [[GNOME]] Desktop and [[KGet]] for the [[KDE]] Desktop.

==History==
Wget is the descendant of an earlier program named '''Geturl''' by the same author, the development of which commenced in late 1995.  The name was changed to ''Wget'' after the author became aware of an earlier [[Amiga]] program named '''GetURL''', written by James Burton in [[AREXX]].

Wget filled a gap in the web downloading software available in the mid-1990s. No single program was able to reliably download files via both [[HTTP]] and [[File Transfer Protocol|FTP]] [[Protocol (computing)|protocols]]. Existing programs either only supported FTP (such as [[NcFTP]] and [ftp://gnjilux.srk.fer.hr/pub/unix/util/dl/ dl]) or were written in [[Perl]], which was not yet ubiquitous at the time. While Wget was inspired by features of some of the existing programs, it aimed to support both HTTP and FTP and to enable the users to build it using only the standard development tools found on every [[Unix]] system.

At that time many Unix users struggled behind extremely slow university and [[Dial-up access|dial-up]] [[Internet]] connections, leading to a growing need for a downloading agent that could deal with transient network failures without assistance from the human operator.

==Features==
===Robustness===
Wget has been designed for robustness over slow or unstable network connections. If a [[download]] does not complete due to a [[computer network|network]] problem, Wget will automatically try to continue the download from where it left off, and repeat this until the whole file has been retrieved. It was one of the first clients to make use of the then-new &lt;code&gt;Range&lt;/code&gt; [[List of HTTP headers|HTTP header]] to support this feature.

===Recursive download===
Wget can optionally work like a [[web crawler]] by extracting resources [[Hyperlink|linked]] from [[HTML]] [[Web page|pages]] and downloading them in sequence, repeating the process [[Recursion|recursively]] until all the  pages have been downloaded or a maximum recursion depth specified by the user has been reached.  The downloaded pages are saved in a directory structure resembling that on the remote server.  This &quot;recursive download&quot; enables partial or complete mirroring of [[web site]]s via HTTP.  Links in downloaded HTML pages can be adjusted to point to locally downloaded material for [[offline]] viewing. When performing this kind of automatic [[mirror (computing)|mirroring]] of web sites, Wget supports the [[Robots Exclusion Standard]] (unless the option &lt;code&gt;-e robots=off&lt;/code&gt; is used).

Recursive download works with [[File Transfer Protocol|FTP]] as well, where Wget issues the &lt;code&gt;LIST&lt;/code&gt; command to find which additional files to download, repeating this process for directories and files under the one specified in the top [[Uniform Resource Locator|URL]].  Shell-like [[Wildcard character|wildcards]] are supported when the download of FTP URLs is requested.

When downloading recursively over either [[HTTP]] or [[File Transfer Protocol|FTP]], Wget can be instructed to inspect the [[timestamps]] of local and remote files, and download only the remote files newer than the corresponding local ones.  This allows easy mirroring of [[HTTP]] and [[File Transfer Protocol|FTP]] sites, but is considered inefficient and more error-prone when compared to programs designed for mirroring from the ground up, such as [[rsync]].  On the other hand, Wget doesn't require special server-side software for this task.

===Non-interactiveness===
Wget is non-interactive in the sense that, once started, it does not require user interaction and does not need to control a [[tty (Unix)|TTY]], being able to log its progress to a separate file for later inspection. That way the user can start Wget and [[log out|log off]], leaving the program unattended. By contrast, most [[Graphical user interface|graphical]] or [[text user interface]] [[web browser]]s require the user to remain logged in and to manually restart failed downloads, which can be a great hindrance when transferring a lot of data.

===Portability===
Written in a highly portable style of [[C (programming language)|C]] with minimal dependencies on third-party libraries, Wget requires little more than a C compiler and a BSD-like interface to [[Transmission Control Protocol|TCP/IP]] networking. Designed as a [[Unix]] program invoked from the [[Unix shell]], the program has been ported to numerous Unix-like environments and systems, including [[Microsoft Windows]] via [[Cygwin]], and Mac OS X.  It is also available as a native [[Microsoft Windows]] program as one of the [[GnuWin]] packages.

===Other features===
*Wget supports download through [[Proxy server|proxies]], which are widely deployed to provide web access inside company [[Firewall (networking)|firewalls]] and to cache and quickly deliver frequently accessed content.
*It makes use of persistent HTTP connections where available.
*[[IPv6]] is supported on systems that include the appropriate interfaces.
*[[Transport Layer Security|SSL/TLS]] is supported for encrypted downloads using the [[OpenSSL]] library.
*Files larger than 2 [[gibibyte|GiB]] are [[Large File Support|supported]] on 32-bit systems that include the appropriate interfaces.
*Download speed may be [[Bandwidth Throttling|throttled]] to avoid using up all of the available [[Bandwidth (computing)|bandwidth]].

==Using Wget==
===Basic usage===
Typical usage of GNU Wget consists of invoking it from the command line, providing one or more URLs as arguments.
&lt;pre&gt;
# Download the title page of example.com to a file
# named &quot;index.html&quot;.
wget http://www.example.com/
&lt;/pre&gt;
&lt;pre&gt;
# Download Wget's source code from the GNU ftp site.
wget ftp://ftp.gnu.org/pub/gnu/wget/wget-latest.tar.gz
&lt;/pre&gt;

More complex usage includes automatic download of multiple URLs into a directory hierarchy.
&lt;pre&gt;
# Download *.gif from a website
# (globbing, like &quot;wget http://www.server.com/dir/*.gif&quot;, only works with ftp)
wget -e robots=off -r -l1 --no-parent -A.gif ftp://www.example.com/dir/
&lt;/pre&gt;
&lt;pre&gt;
# Download the title page of example.com, along with
# the images and style sheets needed to display the page, and convert the
# URLs inside it to refer to locally available content.
wget -p -k http://www.example.com/
&lt;/pre&gt;
&lt;pre&gt;
# Download the entire contents of example.com
wget -r -l 0 http://www.example.com/
&lt;/pre&gt;

===Advanced examples===
Download a mirror of the errata for a book you just purchased, follow all local links recursively and make the files suitable for off-line viewing. Use a random wait of up to 5 seconds between each file download and log the access results to &quot;myLog.log&quot;. When there is a failure, retry for up to 7 times with 14 seconds between each retry. (The command must be on one line.)
&lt;pre&gt;
wget -t 7 -w 5 --waitretry=14 --random-wait -m -k -K -e robots=off 
        http://www.oreilly.com/catalog/upt3/errata/ -o ./myLog.log
&lt;/pre&gt;

Collect only specific links listed line by line in the local file &quot;my_movies.txt&quot;.  Use a random wait of 0 to 33 seconds between files, and use 512 kilobytes per second of [[bandwidth throttling]].  When there is a failure, retry for up to 22 times with 48 seconds between each retry.   Send no tracking [[user agent]] or [[HTTP referer]] to a restrictive site and ignore robot exclusions.  Place all the captured files in the local &quot;movies&quot; directory and collect the access results to the local file &quot;my_movies.log&quot;. Good for downloading specific sets of files without hogging the network:-

Instead of an empty [[HTTP referer|referer]] and [[user agent|user-agent]] use a real one that does not cause an â''ERROR: 403 Forbidden''â message from a restrictive site.  It is also possible to create a '''.wgetrc''' file that holds some default values.&lt;ref&gt;[http://www.askapache.com/dreamhost/wget-header-trick.html Wget Trick to Download from Restrictive Sites]&lt;/ref&gt;
&lt;pre&gt;
wget -t 22 --waitretry=48 --wait=33 --random-wait --referer=&quot;&quot; --user-agent=&quot;&quot;
     --limit-rate=512k -e robots=off -o ./my_movies.log -P./movies -i ./my_movies.txt
&lt;/pre&gt;

To get around cookie tracked sessions:-
&lt;pre&gt;
# Using wget to download content protected by referer and cookies.
# 1. get base url and save its cookies in file
# 2. get protected content using stored cookies
wget --cookies=on --keep-session-cookies --save-cookies=cookie.txt http://first_page
wget --referer=http://first_page --cookies=on --load-cookies=cookie.txt 
     --keep-session-cookies --save-cookies=cookie.txt http://second_page
&lt;/pre&gt;

Mirror and convert CGI, ASP or PHP and others to HTML for offline browsing:-
&lt;pre&gt;
# Mirror website to a static copy for local browsing.
# This means all links will be changed to point to the local files.
# Note --html-extension will convert any CGI, ASP or PHP generated files to HTML (or anything else not .html).
wget --mirror -w 2 -p --html-extension --convert-links -P &lt;dir&gt; http://www.yourdomain.com
&lt;/pre&gt;

==Authors and copyright==
GNU Wget was written by Hrvoje NikÅ¡iÄ with contributions by many other people, including Dan Harkless, Ian Abbott, and Mauro Tortonesi.  Significant contributions are credited in the ''AUTHORS'' file included in the distribution, and all remaining ones are documented in the [[changelog]]s, also included with the program. Wget is currently maintained by Giuseppe Scrivano.&lt;ref&gt;{{cite web |url=http://wget.addictivecode.org/WgetMaintainer |title=WgetMaintainer |date=23 April 2010 |accessdate=20 June 2010}}&lt;/ref&gt;
&lt;!--The previous maintainer was Micah Cowan. --&gt;

The copyright to Wget belongs to the [[Free Software Foundation]], whose policy is to require copyright assignments for all non-trivial contributions to GNU software.&lt;ref&gt;http://www.gnu.org/licenses/why-assign.html&lt;/ref&gt;

===License===
GNU Wget is distributed under the terms of the [[GNU General Public License]], version 3 or later, with a special [[OpenSSL exception|exception]] that allows distribution of binaries [[Linker (computing)|linked]] against the [[OpenSSL]] library. The text of the exception follows:&lt;ref name=&quot;license&quot;/&gt;

&lt;blockquote&gt;
Additional permission under GNU GPL version 3 section 7

&lt;p&gt;If you modify this program, or any covered work, by linking or
combining it with the OpenSSL project's OpenSSL library (or a
modified version of that library), containing parts covered by the
terms of the OpenSSL or SSLeay licenses, the Free Software Foundation
grants you additional permission to convey the resulting work.
Corresponding Source for a non-source form of such a combination
shall include the source code for the parts of OpenSSL used as well
as that of the covered work.
&lt;/blockquote&gt;

It is expected that the exception clause will be removed once Wget is modified to also [[Linker (computing)|link]] with the [[GnuTLS]] library.

Wget's [[documentation]], in the form of a [[Texinfo]] reference manual, is distributed under the terms of the [[GNU Free Documentation License]], version 1.2 or later.  The [[man page]] usually distributed on Unix-like systems is automatically generated from a subset of the Texinfo manual and falls under the terms of the same license.

==Development==
Wget is developed in an open fashion, most of the design decisions typically being discussed on the public mailing list&lt;ref name=&quot;Gmane&quot;&gt;http://news.gmane.org/gmane.comp.web.wget.general&lt;/ref&gt; followed by users and developers. Bug reports and patches are relayed to the same list.

===Source contribution===
The preferred method of contributing to Wget's code and documentation is through source updates in the form of textual [[Patch (Unix)|patches]] generated by the [[diff]] utility.  Patches intended for inclusion in Wget are submitted to the mailing list&lt;ref name=&quot;Gmane&quot;/&gt; where they are reviewed by the maintainers.  Patches that pass the maintainers' scrutiny are installed in the sources.  Instructions on patch creation as well as style guidelines are outlined on the project's wiki.&lt;ref&gt;http://wget.addictivecode.org/PatchGuidelines&lt;/ref&gt;

The source code can also be tracked via a remote [[version control]] repository that hosts revision history beginning with the 1.5.3 release.  The repository is running [[Bazaar (software)|Bazaar]].&lt;ref&gt;{{cite web |url=http://wget.addictivecode.org/RepositoryAccess |title=RepositoryAccess |date=22 May 2010 |accessdate=20 June 2010}}&lt;/ref&gt; Previously, [[Mercurial (software)|Mercurial]] was used. Before that, it had been hosted on [[Subversion (software)|Subversion]], and prior to that via [[Concurrent Versions System|CVS]].

====Release====
When a sufficient number of features or bug fixes accumulate during development, Wget is released to the general public via the GNU FTP site and its mirrors.  Being entirely run by volunteers, there is no external pressure to issue a release nor are there enforceable release deadlines.

Releases are numbered as [[Software versioning|versions]] of the form of ''major.minor[.revision]'', such as ''Wget 1.11'' or ''Wget 1.8.2''.  An increase of the major version number represents large and possibly incompatible changes in Wget's behavior or a radical redesign of the code base.  An increase of the minor version number designates addition of new features and bug fixes.  A new revision indicates a release that, compared to the previous revision, only contains bug fixes.  Revision zero is omitted, meaning that for example Wget 1.11 is the same as 1.11.0. Wget does not use the [[Software_versioning#Odd-numbered_versions_for_development_releases|odd-even release number convention]] popularized by the Linux.

===Notable releases===
The following releases represent notable milestones in Wget's development.  Features listed next to each release are edited for brevity and do not constitute comprehensive information about the release, which is available in the ''NEWS'' file distributed with Wget.&lt;ref&gt;http://svn.dotsrc.org/repo/wget/tags/WGET_1_10/NEWS&lt;/ref&gt;
*Geturl 1.0, released January 1996, was the first publicly available release. The first English-language announcement can be traced to [http://groups-beta.google.com/group/comp.infosystems.www.announce/msg/4268334d269d42ce?hl=en this Usenet news posting], which probably refers to Geturl 1.3.4 released in June.
*Wget 1.4.0, released November 1996, was the first version to use the name ''Wget''. It was also the first release distributed under the terms of the [[GNU GPL]], Geturl having been distributed under an ad-hoc no-[[warranty]] [[Software license|license]].
*Wget 1.4.3, released February 1997, was the first version released as part of [[GNU|the GNU project]] with the copyright assigned to the [[Free Software Foundation|FSF]].
*Wget 1.5.3, released September 1998, was a milestone in the program's popularity.  This version was bundled with many [[GNU/Linux]] based distributions, which exposed the program to a much wider audience.
*Wget 1.6, released December 1999, incorporated many bug fixes for the (by then stale) 1.5.3 release, largely thanks to the effort of Dan Harkless.
*Wget 1.7, released June 2001, introduced [[Transport Layer Security|SSL]] support, [[HTTP cookie|cookies]], and persistent connections.
*Wget 1.8, released December 2001, added [[bandwidth throttling]], new progress indicators, and the breadth-first traversal of the hyperlink [[Graph (mathematics)|graph]].
*Wget 1.9, released October 2003, included experimental [[IPv6]] support, and ability to POST data to HTTP servers.
*Wget 1.10, released June 2005, introduced [[large file support]], [[IPv6]] support on dual-family systems, [[NTLM]] authorization, and [[Transport Layer Security|SSL]] improvements.  The maintainership was picked up by Mauro Tortonesi.
*Wget 1.11, released January 2008, moved to version 3 of the [[GNU General Public License]], and added preliminary support for the &lt;code&gt;Content-Disposition&lt;/code&gt; header, which is often used by [[Common Gateway Interface|CGI]] scripts to indicate the name of a file for downloading. Security-related improvements were also made to the HTTP authentication code. Micah Cowan took over maintainership of the project.
*Wget 1.12, released September 2009, added support for parsing URLs from [[Cascading Style Sheets|CSS]] content on the web, and for handling [[Internationalized Resource Identifier]]s.
*Wget 1.13, released August 2011, support HTTP/1.1, fix some portability issues, use [[GnuTLS]] library by default for secure connections.&lt;ref&gt;[http://bzr.savannah.gnu.org/lh/wget/trunk/annotate/head:/NEWS Wget NEWS file]&lt;/ref&gt;

==Related works==
{{Multiple images
|size = 220
|direction = vertical

|image1 = Gwget-1.0.4.png
|caption1 = Screenshot of GWget 1.0.4 in [[Fedora (operating system)|Fedora]] v12 with [[GNOME]] v2.28.2 installed

|image2 = Kget 4.2.2 phpbb.png
|caption2 = Screenshot of KGet on [[KDE]] v4.2.2, showing an ongoing download
}}

===GWget===
'''GWget''' is a free [[Graphical user interface|graphical]] [[frontend]] for of Wget. It is developed by David SedeÃ±o FernÃ¡ndez and is part of the [[GNOME]] project. GWget supports all of the main features that Wget does, as well as parallel downloads.

===KGet===
{{Main|KGet}}
'''KGet''' is yet another free frontend for [[KDE]]. It is the default download manager of [[Konqueror]], but can also be used with [[Mozilla Firefox]].&lt;ref&gt;[http://fosswire.com/2008/07/23/using-kget-download-manager-with-firefox/ Using KGet Download Manager with Firefox] Peter Upfold (July 23, 2008)FOSSwire&lt;/ref&gt; KGet is part of the KDE Network package.&lt;ref&gt;http://www.freesoftwaremagazine.com/articles/managing_downloads_with_kget&lt;/ref&gt;
{{-}}

==See also==
{{Portal|Free software}}
*[[GNU|The GNU Project]]
*[[cURL]]
*[[NcFTP]]
*[[Web crawler]]
*[[HTTrack]]
*[[lftp]]

==References==
{{Reflist}}

==External links==
*{{Official website|http://www.gnu.org/software/wget}}

{{Download managers}}
{{GNU}}

[[Category:1996 software]]
[[Category:GNU Project software]]
[[Category:HTTP clients]]
[[Category:Free FTP clients]]
[[Category:Free cross-platform software]]
[[Category:Download managers]]
[[Category:Portable software]]
[[Category:Free web crawlers]]
[[Category:Text mode]]

[[ar:ÙØ¬Øª]]
[[ca:GNU Wget]]
[[cs:Wget]]
[[de:Wget]]
[[es:GNU Wget]]
[[fa:Wget]]
[[fr:GNU Wget]]
[[hr:Wget]]
[[id:Wget]]
[[it:Wget]]
[[ja:GNU Wget]]
[[pl:Wget]]
[[pt:Wget]]
[[ru:Wget]]
[[sv:Wget]]
[[uk:Wget]]
[[zh:Wget]]</rev></revisions></page></pages></query></api>
