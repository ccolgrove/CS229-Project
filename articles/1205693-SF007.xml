<?xml version="1.0"?><api><query><pages><page pageid="1205693" ns="0" title="ZFS"><revisions><rev xml:space="preserve">{{Other uses}}
{{Infobox Filesystem
| full_name               = ZFS &lt;!-- No, really. Read the article.  --&gt;
| name                    = ZFS
| developer               = [[Sun Microsystems]]
| introduction_os         = [[OpenSolaris]]
| introduction_date       = November 2005
| partition_id            =
| directory_struct        = Extensible hash table
| file_struct             =
| bad_blocks_struct       =
| max_filename_size       = 255 bytes
| max_files_no            = 2&lt;sup&gt;48&lt;/sup&gt;
| max_volume_size         = 16&amp;nbsp;[[Exbibyte|EiB]]
| max_capacity            = 256&amp;nbsp;[[UB]] (2&lt;sup&gt;128&lt;/sup&gt; bytes)
| max_file_size           = 16&amp;nbsp;[[Exbibyte|EB]] (2&lt;sup&gt;64&lt;/sup&gt; bytes)
| filename_character_set  =
| dates_recorded          =
| date_range              =
| date_resolution         =
| forks_streams           = Yes (called [[Extended Attributes]])
| attributes              = [[POSIX]]
| file_system_permissions = POSIX, NFSv4 ACLs
| compression             = Yes
| data_deduplication      = Yes
| encryption              = Yes&lt;ref name=&quot;encryption&quot; /&gt;
| OS                      = [[Solaris (operating system)|Solaris]], [[OpenSolaris]] , [[Illumos]] distributions , [[OpenIndiana]] , [[FreeBSD]], [[Mac OS X Server#Mac OS X Server 10.5 (Leopard Server)|Mac OS X Server 10.5]], [[NetBSD]], [[Linux]] via ZFS-[[Filesystem in Userspace|FUSE]] or partial native support via 3rd party [[Loadable Kernel Module|kernel module]]&lt;ref&gt;{{cite web | url = http://zfsonlinux.org/faq.html#WhatAboutTheLicensingIssue | title = 1.1 What about the licensing issue? | accessdate = 2010-11-18}}&lt;/ref&gt;}}

In [[computing]], '''ZFS''' is a combined [[file system]] and [[logical volume manager]] designed by [[Sun Microsystems]]. The features of ZFS include data integrity verification  against [[data corruption]] modes, support for high storage capacities, integration of the concepts of filesystem and [[volume (computing)|volume management]], [[Snapshot (computer storage)|snapshots]] and [[copy-on-write]] clones, continuous integrity checking and automatic repair, [[RAID-Z]] and native [[NFSv4]] [[Access control lists|ACL]]s. ZFS is implemented as [[open-source software]], licensed under the [[Common Development and Distribution License]] (CDDL). The ZFS name is a [[trademark]] of [[Oracle Corporation|Oracle]].&lt;ref name=&quot;trademark&quot;&gt;{{cite web
| url = http://www.sun.com/suntrademarks/ | title = Sun Trademarks - ZFS | publisher = Sun Microsystems }}&lt;/ref&gt;

== History ==
ZFS was designed and implemented by a team at Sun led by [[Jeff Bonwick]]. It was announced on September 14, 2004.&lt;ref name=&quot;announce&quot;&gt;{{cite web
| url = http://www.sun.com/2004-0914/feature/ | title = ZFS: the last word in file systems | accessdate = 2006-04-30 | publisher = Sun Microsystems | date = September 14, 2004 |archiveurl = http://web.archive.org/web/20060428092023/http://www.sun.com/2004-0914/feature/ |archivedate = April 28, 2006}}&lt;/ref&gt; Source code for ZFS was integrated into the main trunk of Solaris development on October 31, 2005&lt;ref&gt;{{cite web | url = http://blogs.sun.com/roller/page/bonwick?entry=zfs_the_last_word_in | title = ZFS: The Last Word in Filesystems | author = Jeff Bonwick | work = Jeff Bonwick's Blog |date=October 31, 2005 | accessdate = 2006-04-30 }}&lt;/ref&gt; and released as part of build 27 of [[OpenSolaris]] on November 16, 2005. Sun announced that ZFS was included in the 6/06 update to [[Solaris 10]] in June 2006, one year after the opening of the OpenSolaris community.&lt;ref&gt;{{cite web | url = http://www.sun.com/smi/Press/sunflash/2006-06/sunflash.20060620.1.xml | title = Sun Celebrates Successful One-Year Anniversary of OpenSolaris | publisher = Sun Microsystems | date = June 20, 2006 }}&lt;/ref&gt;

The name originally stood for &quot;Zettabyte File System&quot;.&lt;ref&gt;{{cite web | title = ZFS FAQ at OpenSolaris.org | publisher = Sun Microsystems | accessdate = 2011-05-18 | url = http://hub.opensolaris.org/bin/view/Community+Group+zfs/faq#HWhatdoesZFSstandfor | quote = The largest SI prefix we liked was 'zetta' ('yotta' was out of the question) }}&lt;/ref&gt; A ZFS file system can store up to 256 [[quadrillion]] [[Zettabyte|zettabytes]] (ZB), where a zettabyte is 2&lt;sup&gt;70&lt;/sup&gt; bytes.

=== Version numbers ===
As new features are introduced the version number of the ZPool and Z file system are incremented to designate the format and features available.&lt;ref&gt;{{cite web | url = http://download.oracle.com/docs/cd/E19253-01/819-5461/appendixa-1/index.html | title = Solaris ZFS Administration Guide, Appendix A ZFS Version Descriptions | accessdate = 2011-02-11 | publisher = Oracle Corporation | year = 2010 }}&lt;/ref&gt;&lt;ref&gt;{{cite web | url = http://hub.opensolaris.org/bin/view/Community+Group+zfs/version | title = Version | accessdate = 2010-08-17 | publisher = Sun Microsystems }}&lt;/ref&gt;
Notable ZFS storage pool versions include:
* 10 - Supported by Solaris 10 U7
* 14 - Supported by OpenSolaris 2009.06, FreeBSD 8.1
* 15 - Supported by Solaris 10 10/09 (U8), FreeBSD 8.2
* 17 - Triple Parity RAID-Z
* 19 - Supported by Solaris 10 09/10
* 21 - Deduplication
* 22 - Solaris 10 9/10 (U9)
* 28 - FreeBSD 9.0, OpenIndiana/Illumos, ZFSOnLinux, ZFS-FUSE
* 29 - Solaris 10 8/11 (U10)
* 30 - Encryption support, - not compatible with open implementations, only in closed source for-license Solaris 11 Express release.

== Features ==
=== Data Integrity ===
One major feature that distinguishes ZFS from other file systems is that ZFS is designed from the ground up with a focus on data integrity. That is, protect the user's data on disk, against silent corruption caused by e.g., [[bit rot]], cosmic radiation, current spikes, bugs in disk firmware, ghost writes, etc.

Data Integrity is a high priority in ZFS because recent research shows that none of the currently widespread file systems&amp;nbsp;â such as Ext, [[XFS]], [[JFS (file system)|JFS]], [[ReiserFS]], or [[NTFS]]&amp;nbsp;â nor Hardware RAID provide sufficient protection against such problems.&lt;ref&gt;{{cite web|title=Iron File Systems|url=http://pages.cs.wisc.edu/~vijayan/vijayan-thesis.pdf}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=Parity Lost and Parity Regained|url=http://www.cs.wisc.edu/adsl/Publications/parity-fast08.html}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=An Analysis of Data Corruption in the Storage Stack|url=http://www.cs.wisc.edu/adsl/Publications/corruption-fast08.pdf}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=Impact of Disk Corruption on Open-Source DBMS|url=http://www.cs.wisc.edu/adsl/Publications/corrupt-mysql-icde10.pdf}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.baarf.com/ |title=Baarf.com |publisher=Baarf.com |date= |accessdate=2011-11-04}}&lt;/ref&gt; It is well known that [[RAID#Problems with RAID|Hardware RAID has some issues with data integrity]]. Initial research indicates that ZFS clearly protects data better than earlier solutions.&lt;ref&gt;{{cite web
| url         = http://www.cs.wisc.edu/wind/Publications/zfs-corruption-fast10.pdf
| title       = End-to-end Data Integrity for File Systems: A ZFS Case Study
| author      = Yupu Zhang, Abhishek Rajimwale, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau
| publisher   = Computer Sciences Department, University of Wisconsin
| location    = Madison
| page       = 14
| format      = PDF
| accessdate  = 2010-12-06
}}&lt;/ref&gt;

For ZFS, data integrity is achieved by using a ([[Fletcher's checksum|Fletcher-based]]) checksum or a ([[SHA-2]]) hash throughout the file system tree.&lt;ref name=&quot;endtoend&quot;&gt;{{cite web
 | url = http://blogs.sun.com/bonwick/entry/zfs_end_to_end_data
 | title = ZFS End-to-End Data Integrity
 | first = Jeff | last = Bonwich | date = 2005-12-09
}}&lt;/ref&gt; Each block of data is checksummed and the checksum value is then saved in the pointer to that blockârather than at the actual block itself. Next, the block pointer is checksummed, with the value being saved at ''its'' pointer. This checksumming continues all the way up the file system's data hierarchy to the root node, which is also checksummed, thus creating a [[Hash tree|Merkle tree]].&lt;ref name=&quot;endtoend&quot; /&gt; When a block is accessed, regardless of whether it is data or meta-data, its checksum is calculated and compared with the stored checksum value of what it &quot;should&quot; be. If the checksums match, the data is passed up the programming stack to the process that asked for it. If the values do not match, then ZFS can heal the data if the storage pool has redundancy via ZFS type of [[Disk mirroring|mirror]]ing or [[RAID]].&lt;ref&gt;{{cite web
 | url = http://blogs.sun.com/timc/entry/demonstrating_zfs_self_healing
 | title = Demonstrating ZFS Self-Healing | date= 2009-11-16
 | first = Tim | last = Cook
}}&lt;/ref&gt; If the storage pool consists of a single disk it is possible to provide such redundancy by specifying &quot;copies=2&quot; (or &quot;copies=3&quot;) which means that data will be stored twice (thrice) on the disk, effectively halving (1/3) the storage capacity of the disk.&lt;ref&gt;{{cite web
 | url = http://blogs.sun.com/relling/entry/zfs_copies_and_data_protection
 | title = ZFS, copies, and data protection | date= 2007-05-04
 | first = Richard | last = Ranch
}}&lt;/ref&gt; If redundancy exists, then ZFS fetches the second copy of the data (or recreates it via a RAID recovery mechanism), and recalculates the checksumâhopefully reproducing the original value this time. If the data passes the integrity check, the system can then update the first copy with known-good data so that redundancy can be restored.

ZFS cannot fully protect the user's data when using a hardware RAID controller, as it is not able to perform the automatic self-healing unless it controls the redundancy of the disks and data. ZFS prefers direct, exclusive access to the disks, with nothing in between that interferes. If the user insists on using hardware-level RAID, the controller should be configured as [[JBOD]] mode (i.e. turn off RAID-functionality) for ZFS to be able to guarantee data integrity. Note that hardware RAID configured as JBOD may still detach disks that do not respond in time; and as such may require [[Time-Limited Error Recovery|TLER]]/CCTL/ERC-enabled disks to prevent drive dropouts: http://wdc.custhelp.com/app/answers/detail/a_id/1397/~/difference-between-desktop-edition-and-raid-%28enterprise%29-edition-drives

These limitations do not apply when using a non-RAID controller, which is the preferred method of supplying disks to ZFS. A non-RAID controller is generally called a [[Host adapter|Host Bus Adapter]] (HBA) and allows the [[operating system]] to control timeout and error control, rather than the RAID controller which generally has very strict timeout control.

A modern hard disk devotes a large portion of its capacity to error detection data. Many errors occur during normal usage, but are corrected by the disk's internal software, and thus are not visible to the host software. A tiny fraction of errors are not corrected. For example, a modern Enterprise SAS disk specification estimates this fraction to be one uncorrected error in every 10&lt;sup&gt;16&lt;/sup&gt; bits, or approximately one in every 1.2 [[Petabyte|PB]].&lt;ref&gt;{{cite web|title=Are Fibre Channel and SCSI Drives More Reliable?|url=http://permabit.wordpress.com/2008/08/20/are-fibre-channel-and-scsi-drives-more-reliable/}}&lt;/ref&gt; A smaller fraction of errors are not even detected by the disk firmware or the host operating system. This is known as &quot;silent corruption&quot;. In a recent study, [[CERN]] found this issue to be problematic.&lt;ref&gt;http://indico.cern.ch/getFile.py/access?contribId=3&amp;sessionId=0&amp;resId=1&amp;materialId=paper&amp;confId=13797&lt;/ref&gt;

These problems have not been a serious concern while storage devices remained relatively small and slow. Hence, a user very rarely faced silent corruption, so it was not deemed to be a problem that required a solution. With the advent of larger drives and very fast RAID setups, a user is capable of transferring 10&lt;sup&gt;16&lt;/sup&gt; bits in a sufficiently short time. In particular, ZFS creator Jeff Bonwick stated that the fast database at [[Greenplum]] â a database software company located in San Mateo, California specializing in enterprise data cloud solutions for large-scale data warehousing and analytics â faces silent corruption every 15 minutes,&lt;ref name=&quot;bonwickconversation&quot;&gt;{{cite web
| url         = http://queue.acm.org/detail.cfm?id=1317400
| title       = A Conversation with Jeff Bonwick and Bill Moore
| date        = 2007-11-15
| publisher   = Association for Computing Machinery
| accessdate  = 2010-12-06
}}&lt;/ref&gt; which is one of the reasons that Greenplum now base their fast database solution on ZFS. These large and fast raid setups require new file systems that focus on data integrity. This is one of the design goals of ZFS, as explained by Jeff Bonwick.&lt;ref name=&quot;bonwickconversation&quot; /&gt;

ZFS has no &quot;fsck&quot; repair tool, common on Unix/Linux filesystem, which examines and repairs data. Instead, ZFS has a repair tool called &quot;scrub&quot; which examines and repairs Silent Corruption and other problems. Some differences are:
* fsck must be run on an offline filesystem, which means the filesystem must be unmounted and not useable while being repaired.
* fsck usually only checks metadata (such as the journal log) but never checks the data itself. This means, after an fsck, the data might still be corrupt.
* scrub does not need the ZFS filesystem to be taken offline. scrub is designed to be used on a working, mounted alive filesystem.
* scrub checks everything, including metadata and the data.
The official recommendation from Sun/Oracle is to scrub once every month with Enterprise disks, because they have much higher reliability than cheap commodity disks. If using cheap commodity disks, scrub every week.

However, no system is immune to bugs or hardware not following standards.
&lt;blockquote&gt;
&quot;...For example: FLUSH CACHE should only return, when the cache is flushed. But there are dirt cheap converter chips that sends the FLUSH CACHE to disk, but returns a successful FLUSH CACHE in the same moment back to the OS (of course without having NVRAM on disk or in a controller as this would allow to ignore CACHE FLUSH). Or interface converters reordering commands in really funny ways. By such reordering it may happen, that the uberblock is written to disk, before the rest of the structure has been written to disk...&quot;
http://www.c0t0d0s0.org/archives/6071-No,-ZFS-really-doesnt-need-a-fsck.html
&lt;/blockquote&gt;
Thus, there are known cases where ZFS has had problems. Therefore, as an extra safety measure, it is possible to go back in time by using the &quot;-F&quot; flag with the &quot;zpool&quot; command. ZFS use Copy-On-Write, which means old data is not altered. Whenever data is edited and updated, the old data is always left intact, and only the edits are stored, on a new place on the disk. This means every change can be traced back in time. This allows the user to discard the latest change which caused the problem, and instead go back to an earlier functioning state. This is also how ZFS Snapshots works.

=== Storage pools ===

Unlike traditional file systems, which reside on single devices and thus require a volume manager to use more than one device, ZFS filesystems are built on top of virtual storage pools called ''zpools''. A zpool is constructed of [[virtual device]]s (''vdevs''), which are themselves constructed of [[Device node#Block devices|block devices]]: files, hard drive [[Disk partitioning|partitions]], or entire drives, with the last being the recommended usage.&lt;ref&gt;{{cite web | url = http://download.oracle.com/docs/cd/E19253-01/819-5461/gazdp/index.html | title = Solaris ZFS Administration Guide | publisher = Oracle Corporation | accessdate = 2011-02-11}}&lt;/ref&gt; Block devices within a vdev may be configured in different ways, depending on needs and space available: non-redundantly (similar to [[RAID 0]]), as a mirror ([[RAID 1]]) of two or more devices, as a [[RAID-Z]] (similar to RAID-5) group of three or more devices, or as a RAID-Z2 (similar to RAID-6) group of four or more devices.&lt;ref&gt;{{cite web | url = http://www.solarisinternals.com/wiki/index.php/ZFS_Best_Practices_Guide#RAID-Z_Configuration_Requirements_and_Recommendations | title = ZFS Best Practices Guide | publisher = Solaris Performance Wiki | accessdate = 2007-10-02}}&lt;/ref&gt; In July 2009, triple-parity RAID-Z3 was added to [[OpenSolaris]].&lt;ref&gt;{{cite web
 | url = http://bugs.opensolaris.org/bugdatabase/view_bug.do?bug_id=6854612
 | title = Bug ID: 6854612 triple-parity RAID-Z
 | first = Adam | last = Leventhal
 | publisher = Sun Microsystems | accessdate = 2009-07-17
}}&lt;/ref&gt;&lt;ref&gt;{{cite mailing list
 | url = http://mail.opensolaris.org/pipermail/onnv-notify/2009-July/009872.html
 | title = 6854612 triple-parity RAID-Z
 | date= 2009-07-16  | accessdate=2009-07-17
 | mailinglist = zfs-discuss
 | first = Adam | last = Leventhal
}}&lt;/ref&gt;

Thus, a zpool (ZFS storage pool) is vaguely similar to a computer's RAM. The total RAM pool capacity depends on the number of RAM memory sticks and the size of each stick. Likewise, a zpool consists of one or more vdevs. Each vdev can be viewed as a group of hard disks (or partitions, or files, etc.). Each vdev should have redundancy because if a vdev is lost, then the whole zpool is lost. Thus, each vdev should be configured as RAID-Z1, RAID-Z2, mirror, etc. It is not possible to change the number of drives in an existing vdev (Block Pointer Rewrite will allow this, and also allow defragmentation), but it is always possible to increase storage capacity by adding a new vdev to a zpool. It is possible to swap a drive to a larger drive and resilver (repair) the zpool. If this procedure is repeated for every disk in a vdev, then the zpool will grow in capacity when the last drive is resilvered. A vdev will have the same capacity as the smallest drive in the group. For instance, a vdev consisting of three 500 GB and one 700&amp;nbsp;GB drive, will have a capacity of 4 x 500 GB.

====L2ARC====
In addition, pools can have [[hot spare]]s to compensate for failing disks.  ZFS also supports both read and write caching, for which special devices can be used.  Solid State Devices can be used for the '''L2ARC''', or Level 2 [[adaptive replacement cache]], speeding up read operations, while NVRAM buffered SLC memory can be boosted with [[Electric double-layer capacitor|supercapacitors]] to implement a fast, non-volatile write cache, improving synchronous writes.
Finally, when mirroring, block devices can be grouped according to physical chassis, so that the filesystem can continue in the case of the failure of an entire chassis.

Storage pool composition is not limited to similar devices but can consist of ad-hoc, heterogeneous collections of devices, which ZFS seamlessly pools together, subsequently doling out space to diverse filesystems as needed.  Arbitrary storage device types can be added to existing pools to expand their size at any time.
&lt;ref&gt;{{cite web|url=http://www.sun.com/software/solaris/pdf/solariszfs_solutionbrief.pdf |title=&quot;Solaris ZFS Enables Hybrid Storage PoolsâShatters Economic and Performance Barriers&quot; |publisher=Sun.com |date=2010-09-07 |accessdate=2011-11-04}}&lt;/ref&gt;

The storage capacity of all vdevs is available to all of the file system instances in the zpool. A [[Disk quota|quota]] can be set to limit the amount of space a file system instance can occupy, and a [[disk reservation|reservation]] can be set to guarantee that space will be available to a file system instance.

=== Capacity ===

ZFS is a [[128-bit]] file system, so it can address 1.84 Ã 10&lt;sup&gt;19&lt;/sup&gt; times more data than 64-bit systems such as NTFS. The limitations of ZFS are designed to be so large that they would never be encountered. This was assured by surpassing ''physical'' rather than ''theoretical'' limitationsâthere simply is not enough usable matter on the planet Earth to support a maximized ZFS filesystem. Some theoretical limits in ZFS are:

* 2&lt;sup&gt;48&lt;/sup&gt;&amp;nbsp;â Number of entries in any individual directory&lt;ref&gt;{{cite web | url = http://download.oracle.com/docs/cd/E19253-01/819-5461/zfsover-2/index.html | title = Solaris ZFS Administration Guide | publisher = Oracle Corporation | accessdate = 2011-02-11}}&lt;/ref&gt;
* 16 [[exabyte]]s ({{val|16|e=18|u=bytes}})&amp;nbsp;â Maximum size of a single file
* 16 exabytes&amp;nbsp;â Maximum size of any attribute
* 256 [[zettabyte]]s (2&lt;sup&gt;78&lt;/sup&gt; bytes)&amp;nbsp;â Maximum size of any zpool
* 2&lt;sup&gt;56&lt;/sup&gt;&amp;nbsp;â Number of attributes of a file (actually constrained to 2&lt;sup&gt;48&lt;/sup&gt; for the number of files in a ZFS file system)
* 2&lt;sup&gt;64&lt;/sup&gt;&amp;nbsp;â Number of devices in any zpool
* 2&lt;sup&gt;64&lt;/sup&gt;&amp;nbsp;â Number of zpools in a system
* 2&lt;sup&gt;64&lt;/sup&gt;&amp;nbsp;â Number of file systems in a zpool

=== Copy-on-write transactional model ===

ZFS uses a [[copy-on-write]] [[Transaction processing|transactional]] [[object model]]. All block pointers within the filesystem contain a 256-bit [[checksum]] or 256-bit [[Cryptographic hash function|hash]] (currently a choice between  [[Fletcher's checksum|Fletcher-2]], [[Fletcher's checksum|Fletcher-4]], or [[SHA-256]])&lt;ref&gt;{{cite web
 | title = ZFS On-Disk Specification
 | url = http://opensolaris.org/os/community/zfs/docs/ondiskformat0822.pdf
 | publisher = Sun Microsystems, Inc. | year = 2006
}} See section 2.4.&lt;/ref&gt; of the target block which is verified when the block is read. Blocks containing active data are never overwritten in place; instead, a new block is allocated, modified data is written to it, then any [[metadata]] blocks referencing it are similarly read, reallocated, and written. To reduce the overhead of this process, multiple updates are grouped into transaction groups, and an [[intent log]] is used when synchronous write semantics are required.  The blocks are arranged in a tree, as are their checksums (see [[Merkle signature scheme]]).

=== Snapshots and clones ===

An advantage of copy-on-write is that when ZFS writes new data, the blocks containing the old data can be retained, allowing a [[snapshot (computer storage)|snapshot]] version of the file system to be maintained. ZFS snapshots are created very quickly, since all the data composing the snapshot is already stored; they are also space efficient, since any unchanged data is shared among the file system and its snapshots.

Writeable snapshots (&quot;clones&quot;) can also be created, resulting in two independent file systems that share a set of blocks. As changes are made to any of the clone file systems, new data blocks are created to reflect those changes, but any unchanged blocks continue to be shared, no matter how many clones exist. This is an implementation of the [[Copy-on-write]] principle.

=== Dynamic striping ===

Dynamic [[Data striping|striping]] across all devices to maximize throughput means that as additional devices are added to the zpool, the stripe width automatically expands to include them; thus all disks in a pool are used, which balances the write load across them.

=== Variable block sizes ===

ZFS uses variable-sized blocks of up to 128 kilobytes. The currently available code allows the administrator to tune the maximum block size used as certain workloads do not perform well with large blocks.
If [[data compression]] ([[LZJB]]) is enabled, variable block sizes are used. If a block can be compressed to fit into a smaller block size, the smaller size is used on the disk to use less storage and improve IO throughput (though at the cost of increased CPU use for the compression and decompression operations).

=== Lightweight filesystem creation ===

In ZFS, filesystem manipulation within a storage pool is easier than volume manipulation within a traditional filesystem; the time and effort required to create or resize a ZFS filesystem is closer to that of making a new directory than it is to volume manipulation in some other systems.

=== Cache management ===
ZFS also uses the [[Adaptive replacement cache|ARC]], a new method for Read cache management, instead of the traditional Solaris virtual memory [[page cache]].  For Write cache ZFS employs the Intent Log (ZIL).  ZFS makes allowances for both of these methods to incorporate separate virtual devices to improve the total IOPS.  For Read operations it is the &quot;cache&quot; vdev and for Write operations it is the &quot;log&quot; vdev.&lt;ref&gt;{{cite web|author=|url=http://www.unix.com/man-page/FreeBSD/8/zpool |title=Unix.com |publisher=Unix.com |date=2007-11-13 |accessdate=2011-11-04}}&lt;/ref&gt;

=== Adaptive endianness ===

Pools and their associated ZFS file systems can be moved between different platform architectures, including systems implementing different byte orders. The ZFS block pointer format stores filesystem metadata in an [[endianness|endian]]-adaptive way; individual metadata blocks are written with the native byte order of the system writing the block. When reading, if the stored endianness does not match the endianness of the system, the metadata is byte-swapped in memory.

This does not affect the stored data itself; as is usual in [[POSIX]] systems, files appear to applications as simple arrays of bytes, so applications creating and reading data remain responsible for doing so in a way independent of the underlying system's endianness.

=== Deduplication ===

[[Data deduplication|Deduplication]] capability was added to the ZFS source repository at the end of October 2009.&lt;ref&gt;{{cite web| url= http://blogs.sun.com/bonwick/en_US/entry/zfs_dedup | title=ZFS Deduplication}}&lt;/ref&gt;  The OpenSolaris ZFS development packages have been available since December 3, 2009 (build 128).

Effective use of deduplication requires additional hardware.  ZFS designers recommend 2&amp;nbsp;GB of RAM for every 1&amp;nbsp;TB of storage.  Example: at least 32 GB of memory is recommended for 20&amp;nbsp;TB of storage. &lt;ref&gt;{{cite web | url=http://blogs.sun.com/roch/entry/dedup_performance_considerations1 |title=Dedup Performance Considerations }}&lt;/ref&gt; If RAM is lacking, consider adding an [[Solid-state drive|SSD]] as a cache, which will automatically handle the large de-dupe tables. This can speed up de-dupe performance 8x or more.  Insufficient physical memory or lack of ZFS cache results in virtual memory thrashing, which lowers performance.

As of today with Solaris 11 Express, deduplication can cause several problems if you are not aware of the dedup limitations. &lt;ref&gt;{{cite web |url=http://mail.opensolaris.org/pipermail/zfs-discuss/2011-July/049209.html |title=&amp;#91;zfs-discuss&amp;#93; Summary: Dedup memory and performance (again, again) |author=Edward Ned Harvey|date=July 2011|publisher=zfs-discuss mailing list}}&lt;/ref&gt;

=== Encryption ===
The encryption capability in ZFS&lt;ref&gt;{{cite web | url=http://download.oracle.com/docs/cd/E19963-01/html/821-1448/gkkih.html | title=Encrypting ZFS File Systems}}&lt;/ref&gt; is embedded into the I/O pipeline. During writes a block may be compressed, encrypted, checksummed and then deduplicated in that order.  The policy for encryption is set at the dataset level when datasets (file systems or ZVOLs) are created.  The wrapping keys provided by the user/administrator can be changed at any time without taking the file system off line.  The default behaviour is for the wrapping key to be inherited by any child data sets.  The data encryption keys are randomly generated at dataset creation time.  Only descendant datasets (snapshots and clones) share data encryption keys.&lt;ref&gt;{{cite web | url=http://blogs.sun.com/darren/entry/compress_encrypt_checksum_deduplicate_with | title=Having my secured cake and Cloning it too (aka Encryption + Dedup with ZFS)}}&lt;/ref&gt;  A command to switch to a new data encryption key for the clone or at any time is provided&amp;nbsp;â this does not re-encrypt already existing data.

=== Additional capabilities ===

* Explicit I/O priority with deadline scheduling.
* Claimed globally optimal I/O sorting and aggregation.
* Multiple independent prefetch streams with automatic length and stride detection.
* Parallel, constant-time directory operations.
* End-to-end checksumming, using a kind of &quot;[[Data Integrity Field]]&quot;, allowing data corruption detection (and recovery if you have redundancy in the pool).
* Transparent filesystem compression. Supports [[LZJB]] and [[gzip]].&lt;ref&gt;{{cite web | title =  Solaris ZFS Administration Guide | work = Chapter 6 Managing ZFS File Systems | accessdate = 2009-03-17 | url = http://download.oracle.com/docs/cd/E19963-01/821-1448/gavwq/index.html}}{{dead link|date=November 2011}}&lt;/ref&gt;
* Intelligent [[Data scrubbing|scrubbing]] and [[Disk mirroring|resilvering]] (resyncing).&lt;ref&gt;{{cite web | title = Smokin' Mirrors | work = Jeff Bonwick's Weblog | date = 2006-05-02 | accessdate = 2007-02-23 | url = http://blogs.sun.com/bonwick/entry/smokin_mirrors}}&lt;/ref&gt;
* Load and space usage sharing among disks in the pool.&lt;ref&gt;{{cite web | title = ZFS Block Allocation | work = Jeff Bonwick's Weblog | date = 2006-11-04 | accessdate = 2007-02-23 | url = http://blogs.sun.com/bonwick/entry/zfs_block_allocation}}&lt;/ref&gt;
* Ditto blocks: Configurable data replication per filesystem, with zero, one or two extra copies requested per write for user data, and with that same base number of copies plus one or two for metadata (according to metadata importance).&lt;ref&gt;{{cite web | title =  Ditto Blocks&amp;nbsp;â The Amazing Tape Repellent | work = Flippin' off bits Weblog | date = 2006-05-12 | accessdate = 2007-03-01 | url = http://blogs.sun.com/bill/entry/ditto_blocks_the_amazing_tape}}&lt;/ref&gt; If the pool has several devices, ZFS tries to replicate over different devices. Ditto blocks are primarily an additional protection against corrupted sectors, not against total disk failure.&lt;ref name=&quot;ditto-block-behavior&quot;&gt;{{cite web | url = http://opensolaris.org/jive/thread.jspa?messageID=417776 | title = Adding new disks and ditto block behaviour | accessdate = 2009-10-19 }}&lt;/ref&gt;
* ZFS design (copy-on-write + superblocks) is safe when using disks with write cache enabled, if they honor the write barriers. This feature provides safety and a performance boost compared with some other filesystems.
* When entire disks are added to a ZFS pool, ZFS automatically enables their write cache. This is not done when ZFS only manages discrete slices of the disk, since it does not know if other slices are managed by non-write-cache safe filesystems, like [[Unix File System|UFS]].
* Per-user and per-group quotas support.&lt;ref name=&quot;per-user-quotas&quot;&gt;{{cite web | url = http://www.opensolaris.org/os/community/zfs/version/15/ | title = OpenSolaris.org | publisher = Sun Microsystems | accessdate = 2009-05-22}}&lt;/ref&gt;
* Filesystem encryption since Solaris 11 Express&lt;ref name=&quot;encryption&quot;&gt;{{cite web | url = http://www.oracle.com/technetwork/server-storage/solaris11/documentation/solaris-express-whatsnew-201011-175308.pdf | title =What's new in Solaris 11 Express 2010.11 | publisher = Oracle | accessdate = 2010-11-17}}&lt;/ref&gt;
* Pools can be imported readonly
* At import time a recovery by rolling back whole transactions is possible.
* ''Planned features:''
** The so-called Block Pointer rewrite functionality is due to be added in the same time frame, paving the way for resizing pools, defragmentation, (re-)applying compression on filesystems and so on.&lt;ref&gt;{{cite web|url=http://blogs.sun.com/video/entry/kernel_conference_australia_2009_jeff |title=Jeff Bonwick Keynote at Kernel Conference Australia 2009 |publisher=Blogs.sun.com |date=2009-09-28 |accessdate=2011-11-04}}&lt;/ref&gt;

== Limitations ==

* Capacity expansion is normally achieved by adding groups of disks as a top-level vdev: simple device, RAID-Z, RAID-Z2, RAID-Z3, or mirrored. Newly written data will dynamically start to use all available vdevs. It is also possible to expand the array by iteratively swapping each drive in the array with a bigger drive and waiting for ZFS to heal itself&amp;nbsp;â the heal time will depend on the amount of stored information, not the disk size. The new free space will not be available until all the disks have been swapped.
* It is currently not possible to reduce the number of top-level vdevs in a pool nor otherwise reduce pool capacity.&lt;ref&gt;{{cite web
 | url = http://bugs.opensolaris.org/view_bug.do?bug_id=4852783
 | title = Bug ID 4852783: reduce pool capacity
 | publisher = OpenSolaris Project | accessdate = 2009-03-28
}}&lt;/ref&gt; This functionality was said to be in development already in 2007.&lt;ref&gt;{{cite mailing list
 | url = http://mail.opensolaris.org/pipermail/zfs-discuss/2007-April/010384.html
 | title = Permanently removing vdevs from a pool
 | mailinglist = zfs-discuss | date = 2007-04-19
 | first = Mario | last = Goebbels
}}&lt;/ref&gt; It is not available as of Solaris 10 9/10 (AKA update 9).
* It is not possible to add a disk as a column to a [[RAID-Z]], RAID-Z2, or RAID-Z3 vdev. This feature depends on the block pointer rewrite functionality due to be added soon. One can however create a new RAID-Z vdev and add it to the zpool.
* Vdevs cannot be nested, so a mirror or RAID-Z top-level vdev can only contain files or disks. Mirrors of mirrors (or other combinations) are not allowed.
* Reconfiguring the number of devices in a top-level vdev requires copying data offline, destroying the pool, and recreating the pool with the new top-level vdev configuration, except for adding extra redundancy to an existing mirror, which can be done at any time or if all top level vdevs are mirrors with sufficient redundancy the zpool split&lt;ref&gt;{{cite web|url=http://download.oracle.com/docs/cd/E19253-01/816-5166/zpool-1m/?l=en&amp;n=1&amp;a=view |title=zpool(1M) |publisher=Download.oracle.com |date=2010-06-11 |accessdate=2011-11-04}}&lt;/ref&gt; command can be used to remove a vdev from each top level vdev in the pool, creating a 2nd pool with identical data.
* If you use a single disk, by default, ZFS can only detect and report silent data corruption errors (because of the checksums) but not repair the errors. For ZFS to be able to both detect and also repair the data corruption, you must specify &quot;copies=2&quot; http://blogs.sun.com/relling/entry/zfs_copies_and_data_protection which tells ZFS to store data twice on the disk (halving your storage capacity). If a data block gets corrupted, ZFS will repair the data block from another copy. Of course, &quot;copies&quot; does not help against a disk crash. To recover from a disk crash, you need disk redundancy such as raidz1, raidz2 or mirror. This applies to all file systems; no file system can protect your data against a disk crash when you use a single disk. You need two or more disks. Thus, ZFS &quot;copies&quot; is not a limitation, but a great advantage because of ZFS' ability to repair corrupted data even when using only a single disk. To further increase safety, &quot;copies=3&quot; can be used, which stores data thrice on every disk.
* Resilver (repair) of a crashed disk in a ZFS raid takes a long time. This applies to all types of RAID, in one way or another. This means that future large disks, say 5&amp;nbsp;TB or 6&amp;nbsp;TB, can take several days to repair. This means that raidz1 (similar to RAID-5) should be avoided, because repairing a raid puts additional stress on the other disks which might cause them to crash, losing all data in the storage pool if configured as raidz1. Therefore, with large disks one should use raidz2 (allow two disks to crash) or raidz3 (allow three disks to crash). Adam Leventhal explains this problem further http://dtrace.org/blogs/ahl/2009/07/21/triple-parity-raid-z/. It should be noted, however, that ZFS RAID differs from conventional RAID solutions by only  reconstructing the data when replacing a disk, not the entirety of the disk, which means that replacing a member disk on a ZFS pool that is half full will take only half the time as compared to conventional RAID.
* [[IOPS]] performance of a ZFS storage pool can suffer if the ZFS raid is not appropriately configured. This applies to all types of RAID, in one way or another. If the zpool consists of only one group of disks configured as, say, raidz2 - then the IOPS performance will be that of a single disk. This means, to get high IOPS performance, the zpool should consist of several vdevs, because one vdev gives the IOPS of a single disk. However, there are ways to mitigate this IOPS performance problem, for instance add SSDs as L2ARC cache&amp;nbsp;â which can boost IOPS into 100.000s http://blogs.sun.com/brendan/entry/a_quarter_million_nfs_iops . In short, a zpool should consist of several groups of vdevs, each vdev consisting of 8-12 disks. It is not recommended to create a zpool with a single large vdev, say 20 disks, because IOPS performance will be that of a single disk, which also means that resilver time will be very long (possibly weeks with future large drives).

== Platforms ==

=== Solaris 10 ===

ZFS is part of Sun's own Solaris operating system and is thus available on both [[SPARC]] and [[x86]]-based systems. Since the code for ZFS is open source, a port to other operating systems and platforms can be produced without Sun's involvement.

=== Solaris 11 ===

After Oracle's Solaris 11 Express release, the OS/Net consolidation (the main OS code) was made proprietary and closed-source, and further ZFS upgrades and implementations inside Solaris (such as encryption) are not compatible with other non-proprietary implementations which use previous versions of ZFS.

When creating a new ZFS pool, to retain the ability to use access the pool from other non-proprietary Solaris-based distributions, it is recommended to upgrade to Solaris 11 Express from OpenSolaris (snv_134b), and thereby stay at ZFS version 28.

=== OpenSolaris ===

[[OpenSolaris]] 2008.05 and 2009.06 use ZFS as their default filesystem. There are over a dozen 3rd party distributions, [http://www.opensolaris.org/os/community/distribution/links/ of which nearly a dozen are mentioned here.] ([[OpenIndiana]] and [[Illumos]] are two new distributions not included on the OpenSolaris distribution reference page.)

=== OpenIndiana ===

[[OpenIndiana]] 148 and 151 use ZFS version 28, as implemented in [[Illumos]].

By upgrading from OpenSolaris snv_134 to both OpenIndiana and Solaris 11 Express, one also has the ability to upgrade and separately boot Solaris 11 Express on the same ZFS pool, but one should not install Solaris 11 Express first because of ZFS incompatibilities introduced by Oracle past ZFS version 28.&lt;ref name=istgt&gt;{{cite web|url=http://wiki.openindiana.org/oi/Upgrading+from+OpenSolaris|title=Upgrading from OpenSolaris|accessdate=24 September 2011}}&lt;/ref&gt;

=== FreeBSD ===
Pawel Jakub Dawidek ported ZFS to [[FreeBSD]], and it has been part of FreeBSD since version 7.0.&lt;ref name=fbsd&gt;{{cite web | url = http://lists.freebsd.org/pipermail/freebsd-current/2007-April/070544.html | title = ZFS committed to the FreeBSD base | first = Pawel | last = Dawidek | accessdate = 2007-04-06 | date = April 6, 2007}}&lt;/ref&gt; This includes zfsboot, which allows booting FreeBSD directly from a ZFS volume.&lt;ref name=fbsd-zfs13&gt;{{cite web | url = http://svn.freebsd.org/viewvc/base?view=revision&amp;revision=192498 | title = Revision 192498 | accessdate = 2009-05-22 | date = May 20, 2009}}&lt;/ref&gt;&lt;ref name=fbsd-zfs13voras&gt;{{cite web | url = http://ivoras.sharanet.org/blog/tree/2009-05-21.zfs-v13-in-7-stable.html | title = ZFS v13 in 7-STABLE | accessdate = 2009-05-22 | date = May 21, 2009}}&lt;/ref&gt;

FreeBSD's ZFS implementation is fully functional; the only missing features are kernel [[CIFS]] server and [[iSCSI]], but at least the latter can be added using externally available packages.&lt;ref name=autogenerated1&gt;{{cite web|url=http://shell.peach.ne.jp/aoyama/|title=iSCSI target for FreeBSD|accessdate=6 August 2011}}&lt;/ref&gt; A CIFS server can be emulated in user space using [[Samba (software)|Samba]].

FreeBSD 7-stable (where updates to the series of versions 7.x are committed to) uses zpool version 6.

FreeBSD version 8 includes a much-updated implementation of ZFS, and zpool version 13 is supported in FreeBSD release 8.0.&lt;ref name=&quot;fbsdrl8&quot;&gt;
{{cite web
| url         = http://www.freebsd.org/releases/8.0R/relnotes.html
| title       = FreeBSD 8.0-RELEASE Release Notes
| publisher   = FreeBSD
| accessdate  = 2009-11-27
}}&lt;/ref&gt;  zpool version 14 support was added to the 8-stable branch on 11 January 2010,&lt;ref name=&quot;fbsd8szfs14&quot;&gt;
{{cite web
| url         = http://svn.freebsd.org/viewvc/base/stable/8/cddl/contrib/opensolaris/cmd/?view=log
| title       = FreeBSD 8.0-STABLE Subversion logs
| publisher   = FreeBSD
| accessdate  = 2010-02-05
}}&lt;/ref&gt; and is included in FreeBSD release 8.1. zpool version 15 is supported in release 8.2.&lt;ref name=&quot;fbsd8szfs15&quot;&gt;
{{cite web
| url         = http://www.freebsd.org/releases/8.2R/relnotes.html
| title       = FreeBSD 8.2-RELEASE Release Notes
| publisher   = FreeBSD
| accessdate  = 2011-03-09
}}&lt;/ref&gt;
The 8-stable branch gained support for zpool version v28 and zfs version 5 in early June 2011.&lt;ref name=fbsd-8stable-zfs28-announcement&gt;{{cite web | url = http://lists.freebsd.org/pipermail/freebsd-stable/2011-June/062900.html | title = HEADS UP: ZFS v28 merged to 8-STABLE | accessdate = 2011-06-11 | date = June 6, 2011}}&lt;/ref&gt; Therefore, v28 will be supported in the 8.x FreeBSD series with the release of FreeBSD 8.3.

The 9-current development branch of FreeBSD uses ZFS Pool version 28.

=== FreeNAS ===
[[FreeNAS]], an embedded open source [[network-attached storage]] (NAS) distribution based on [[FreeBSD]], has the same ZFS support as FreeBSD.

=== GNU/kFreeBSD ===

Being based on the FreeBSD kernel, [[GNU/kFreeBSD]] has ZFS support from the kernel.  However, it depends on the distribution of GNU/kFreeBSD whether the necessary userland tools are available. The only distribution of this system to the date ([[Debian GNU/kFreeBSD]]) provides ZFS utilities in the [http://packages.debian.org/sid/zfsutils zfsutils package]. Additionally, the Debian installer supports installing the operating system under ZFS on the [[amd64]] architecture.

=== NetBSD ===
The NetBSD ZFS port was started as a part of the 2007 [[Google Summer of Code]] and in August 2009 the code was merged into [[NetBSD]]'s source tree.&lt;ref name=netbsd&gt;{{cite web | url = http://www.netbsd.org/contrib/soc-projects.html#zfs-port | title = NetBSD Google Summer of Code projects: ZFS }}&lt;/ref&gt;

=== Mac OS X ===
The first indication of [[Apple Inc.]]'s interest in ZFS was an April 2006 post on the opensolaris.org zfs-discuss mailing list where an Apple employee mentioned being interested in porting ZFS to their [[Mac OS X]] operating system.&lt;ref&gt;{{cite web | url = http://mail.opensolaris.org/pipermail/zfs-discuss/2006-April/002119.html | title = Porting ZFS to OSX | work = zfs-discuss | date = April 27, 2006 | accessdate = 2006-04-30 }}&lt;/ref&gt;

In the release version of Mac OS X 10.5, ZFS was available in read-only mode from the command line, which lacks the possibility to create zpools or write to them.&lt;ref&gt;{{cite web | url=http://www.macnn.com/articles/07/06/12/leopard.uses.hfs.not.zfs/ | title=Apple: Leopard offers limited ZFS read-only | work = MacNN | date = June 12, 2007 | accessdate = 2007-06-23 }}&lt;/ref&gt; Before the 10.5 release, Apple released the &quot;ZFS Beta Seed v1.1&quot;, which allowed read-write access and the creation of zpools,&lt;ref&gt;{{cite web | url=http://arstechnica.com/journals/apple.ars/2007/10/07/apple-delivers-zfs-readwrite-developer-preview-1-1-for-leopard | title=Apple delivers ZFS Read/Write Developer Preview 1.1 for Leopard | work = Ars Technica | date = October 7, 2007 | accessdate = 2007-10-07 }}&lt;/ref&gt; however the installer for the &quot;ZFS Beta Seed v1.1&quot; has been reported to only work on version 10.5.0, and has not been updated for version 10.5.1 and above.&lt;ref&gt;{{cite web | url=http://synesius.wordpress.com/2007/11/18/zfs-beta-seed-v11-will-not-install-on-leopard1-1051/ | title=ZFS Beta Seed v1.1 will not install on Leopard.1 (10.5.1) &quot; ideas are free | author = ChÃ© Kristo | date = November 18, 2007 | accessdate = 2007-12-30 }}{{dead link|date=November 2011}}&lt;/ref&gt;

In August 2007, Apple opened a ZFS project on their [[Mac OS Forge]] site. On that site, Apple provided the source code and binaries of their port of ZFS which includes read-write access, but there was no installer available&lt;ref&gt;[http://zfs.macosforge.org/ ZFS.macosforge.org]&lt;/ref&gt; until a third-party developer created one.&lt;ref&gt;http://alblue.blogspot.com/2008/11/zfs-119-on-mac-os-x.html |title=Alblue.blogspot.com&lt;/ref&gt;

In October 2009, Apple announced a shutdown of the ZFS project on Mac OS Forge.  No explanation was given, just the following statement: &quot;The ZFS project has been discontinued. The mailing list and repository will also be removed shortly.&quot; Versions of the previously released source and binaries, as well as the wiki, have been preserved and development has been adopted by a group of enthusiasts.&lt;ref name=&quot;code.google.com&quot;&gt;{{cite web|url=http://code.google.com/p/maczfs/ |title=maczfs - Support and ongoing development for the Mac port of ZFS - Google Project Hosting |publisher=Code.google.com |date= |accessdate=2011-11-04}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://groups.google.com/group/zfs-macos/?pli=1 |title=zfs-macos &amp;#124; Google Groups |publisher=Groups.google.com |date= |accessdate=2011-11-04}}&lt;/ref&gt;

Complete ZFS support was once advertised as a feature of Snow Leopard Server ([[Mac OS X Server]] 10.6). However, all references to this feature have been silently removed; it is no longer listed on the Snow Leopard Server features page.&lt;ref&gt;{{cite web | url = http://www.apple.com/server/macosx/snowleopard/ | title = Snow Leopard | date = June 9, 2009 | accessdate = 2008-06-10}}&lt;/ref&gt; Apple has not commented regarding the omission.

The [http://code.google.com/p/maczfs/ maczfs] project mirrored the public archives before they disappeared,&lt;ref name=&quot;code.google.com&quot;/&gt; and a community-maintained project currently (as of 5 May 2011) provides basic ZFS software for most recent versions of OS X, including Lion (10.7)&lt;ref name=&quot;code.google.com&quot; /&gt;

In March 2011, the company Ten's Complement LLC  (founded by Don Brady, a former Apple engineer who was technical lead on the original HFS+ team and worked on Apple's abandoned internal project to port ZFS&lt;ref name=&quot;arstechnica.com&quot;&gt;{{cite web | url = http://arstechnica.com/apple/news/2011/03/how-zfs-is-slowly-making-its-way-to-mac-os-x.ars | title = How ZFS is slowly making its way to Mac OS X | date = March 18, 2011 | accessdate = 2011-03-19}}&lt;/ref&gt;) announced that it was close to releasing a version of ZFS for Mac OS X called &quot;Z{{nbhyph}}410 Storage&quot;.&lt;ref&gt;ZDNet.com: [http://www.zdnet.com/blog/apple/zfs-returns-to-the-mac/9782 ZFS returns to the Mac] March 14, 2011&lt;/ref&gt; Z{{nbhyph}}410 Storage would be targeted at [[prosumer]]s.&lt;ref&gt;âWe think the market for Mac OS X server is in serious decline (...) There's a huge chasm between using Xsan over Fibre Channel and a USB drive with Time Machine,&quot; Brady told Ars. &quot;That middle piece is what we're looking atâusers that want the convenience of a device like a Drobo, but with more reliability and [easy verifiability].â&lt;/ref&gt; As of 15 August 2011 Z-410 is in a private beta state and supports zpool v28. &lt;ref&gt;http://twitter.com/#!/TensComplement/status/102817428173635584&lt;/ref&gt;

=== Linux ===

Porting ZFS to [[Linux]] is complicated by the fact that the [[GNU General Public License]], which governs the [[Linux kernel]], is incompatible with the Sun [[CDDL]] under which ZFS is distributed.  According to some developers a single derived work of both projects cannot be legally distributed, as it is not possible to simultaneously meet both licenses' requirements.&lt;ref&gt;{{cite web|author= |url=http://lwn.net/Articles/237905/ |title=Linus on GPLv3 and ZFS |publisher=Lwn.net |date=2007-06-12 |accessdate=2011-11-04}}&lt;/ref&gt; To include ZFS in the Linux kernel it would have to be cleanly reimplemented, and patents may hamper this.&lt;ref name=kerneltrap&gt;{{cite web |url = http://kerneltrap.org/node/8066 | title = Linux: ZFS, Licenses and Patents | author = Jeremy Andrews | accessdate = 2007-04-21 | date = April 19, 2007 }}&lt;/ref&gt;

==== Linux FUSE ====

Another solution to this problem was to port ZFS to Linux's [[FUSE (Linux)|FUSE]] system so the [[filesystem]] runs in [[userspace]] instead, where it is not considered a derived work of the kernel. A project to do this was sponsored by Google's [[Google Summer of Code|Summer of Code]] program in 2006.&lt;ref name=fuse&gt;{{cite web |url = http://zfs-on-fuse.blogspot.com/ | title = ZFS on FUSE/Linux | author = Ricardo Correia | accessdate = 2009-03-16 | date = March 16, 2009 }}&lt;/ref&gt; The original [http://www.wizy.org/wiki/ZFS_on_FUSE ZFS on FUSE project is available here.] Development for ZFS on FUSE/Linux now takes place at [http://zfs-fuse.net/ zfs-fuse.net].

==== Native ZFS on Linux ====
A native port of ZFS for Linux is [http://zfsonlinux.org/ in development].  This ZFS on Linux port was produced at the [[Lawrence Livermore National Laboratory]] (LLNL) under Contract No. DE-AC52-07NA27344 (Contract 44) between the U.S. Department of Energy (DOE) and Lawrence Livermore National Security, LLC (LLNS) for the operation of LLNL. It has been approved for release under LLNL-CODE-403049.  The port is currently in release candidate status for version 0.6.0, which supports mounting filesystems.

Another native port was being worked on by [http://www.kqstor.com/ KQ Infotech ].&lt;ref name=linuxforums&gt;{{cite web |url = http://www.linuxforums.co.uk/viewtopic.php?f=16&amp;t=3 | title = ZFS Port to Linux (all versions) | author = Darshin | accessdate = 2010-08-31| date = August 24, 2010 }}&lt;/ref&gt;&lt;ref&gt; {{cite web | url = http://www.stec-inc.com/press/kqi/supportfaq.php | title =Where can I get the ZFS for Linux source code?| author = kqi | date = 2011--6-12}} &lt;/ref&gt;  This port used the LLNL ZVOL implementation as a starting point. A GA release supporting zpool v28 was released in January 2011.&lt;ref name=phoronixbenchmark&gt;{{cite web | url=http://www.phoronix.com/scan.php?page=article&amp;item=linux_kqzfs_benchmarks&amp;num=1 | title = Running The Native ZFS Linux Kernel Module, Plus Benchmarks | author=Phoronix | accessdate=2010-12-07 | date=November 22, 2010 }}&lt;/ref&gt; In mid-2011, KQ Infotech was acquired by another company, and as such their work on ZFS had ceased.&lt;ref name=phoronix&gt;{{cite web | url=http://www.phoronix.com/scan.php?page=news_item&amp;px=OTU1NQ | title = KQ ZFS Linux Is No Longer Actively Being Worked On | date=June 10, 2011 }}&lt;/ref&gt; Their code can be found on [[github]].&lt;ref name=zfs&gt;{{cite web | url=https://github.com/zfs-linux/zfs | title = zfs-linux / zfs}}&lt;/ref&gt;

=== Comparisons ===
List of Operating Systems, Distros and add-ons that support ZFS, the zpool version it supports, and the Solaris build they are based on (if any):
{| class=&quot;wikitable sortable&quot; style=&quot;font-size: 85%; text-align: center; width: auto&quot;
|-
! OS
!Zpool version
! Sun/Oracle Build #
! Comments
|-
| Oracle Solaris Express 11 2010.11
| 31
| b151a
| licensed for testing only
|-
| [[OpenSolaris]] 2009.06
| 14
| b111b
|
|-
| [[OpenSolaris]] (last dev)
| 22
| b134
|
|-
| [[OpenIndiana]]
| 28
| b147
| OpenIndiana creates a name clash with naming their code b151a
|-
| [[Nexenta]] Core 3.0.1
| 26
| b134+
| GNU userland
|-
| [[NexentaStor]] Community 3.1.0
| 28
| b134+
| GNU userland
|-
| [[NexentaStor]] Community 3.0.1
| 26
| b134+
| up to 18&amp;nbsp;TB, web admin
|-
| NexentaStor Enterprise
| 28
| b134 +
| not free, web admin
|-
| [[FreeBSD]] 8.2-RELEASE
| 15
|
| no CIFS or iSCSI
|-
| [[FreeBSD]] 8-STABLE / 9-CURRENT
| 28
|
| no CIFS or iSCSI
|-
| Linux FUSE 0.7.0
| 23
|
| low efficiency
|-
| Native Linux port (LLNL)
| 28
|
| no stable POSIX layer, release candidate has basic POSIX layer
|-
| Native Linux port (KQ Infotech)
| 28
|
| includes POSIX layer
|-
| [[Belenix]] 0.8b1
| 14
| b111
|
|-
| [[Schillix]] 0.7.2
| 28
| b147
|
|-
| StormOS &quot;hail&quot;
|
|
| based on Nexenta
|-
| Jaris
|
|
| Japanese
|-
| [[MilaX]] 0.5
| 20
| b128a
| small size
|-
| FreeNAS 8.0.2
| 15
| 
| 
|-
| Korona 4.5.0
| 22
| b134
| KDE
|-
| EON NAS
| 22
| b130
| embedded NAS
|-
| [[Mac OS X]] 10.6 (kernel extension / module)
| 8
|
| Somewhat stable with installable packages for those who wish to use it and test, 1 reported crash. [http://code.google.com/p/maczfs/ Project Page]
|-
| [[Mac OS X]] 10.6/10.7 (Z-410)
|
|
| Commercial/nonfree port. Currently (November 2011) in beta. [http://tenscomplement.com/z-410-storage-main-features]
|}
(updated 2011/11/26)

== See also ==
{{Portal|Free software}}
* [[Btrfs]]&amp;nbsp;â for Linux
* [[Comparison of file systems]]
* [[Ext4]]
* [[HAMMER]]&amp;nbsp;â a file system with a similar feature set for [[DragonFly BSD]]
* [[Log-structured File System (BSD)|LFS]]&amp;nbsp;â BSD Log Structured Filesystem
* [[List of file systems]]
* [[Logical Volume Manager (Linux)|LVM]]&amp;nbsp;â Logical Volume Manager (Linux), supports snapshots
* [[LZJB]]&amp;nbsp;â data compression algorithm used in ZFS
* [[NILFS]]&amp;nbsp;â a Linux filesystem with checksumming (but not [[Data scrubbing|scrubbing]]), also supporting snapshots
* [[Reiser4]]
* [[Sun Open Storage]]
* [[Veritas File System]] and [[Veritas Volume Manager]]&amp;nbsp;â similar to ZFS
* [[Versioning file system]]s&amp;nbsp;â List of versioning file systems
* [[Write Anywhere File Layout]]&amp;nbsp;â a similar file system by [[NetApp]]

== References ==
{{Reflist|30em}}

== Bibliography ==
{{Refbegin}}
* {{Cite document
| first1      = Scott
| last1       = Watanabe
| date        = November 23, 2009
| title       = Solaris ZFS Essentials
| edition     = 1st
| publisher   = [[Prentice Hall]]
| pages       = 256
| isbn        = 0137000103
| url         = http://www.informit.com/store/product.aspx?isbn=0137000103
| format      = {{Dead link|date=June 2010}}
| postscript      = &lt;!--None--&gt;
}}
{{Refend}}

== External links ==
* [http://www.opensolaris.org/os/community/zfs/ ZFS Development Community and detailed ZFS Documentation]
* [http://www.solarisinternals.com/wiki/index.php/ZFS_Best_Practices_Guide ZFS Best Practices Guide]
* [http://www.opensolaris.org/os/community/zfs/docs/zfs_last.pdf ZFS The Last word in File Systems]
* [http://www.opensolaris.org/os/community/zfs/boot/ ZFS Boot Project]
* [http://www.opensolaris.org/os/project/zfs-crypto ZFS Encryption Project]
* [http://www.sun.com/software/media/real/zfs_learningcenter/high_bandwidth.html Sun's ZFS Learning Center Presentations on ZFS by Bill Moore]
* [http://www.i-justblog.com/2009/08/zfs-tip-comparison-of-svm-mirroring-and.html Comparison of SVM mirroring and ZFS mirroring]
* [http://sites.google.com/site/eonstorage/ EON ZFS Storage (NAS) distribution]
* [http://www.zfsonlinux.org/ ZFS on Linux Homepage]
* [http://www.usenix.org/events/fast10/tech/full_papers/zhang.pdf End-to-end Data Integrity for File Systems: A ZFS Case Study]
* [http://www.imm.dtu.dk/English/Research/Embedded_Systems_Engineering/Publications.aspx?lg=showcommon&amp;id=266538 Storage systems and ZFS in FenixOS]

{{Solaris}}
{{FreeBSD}}
{{Mac OS X}}
{{Filesystem}}

{{DEFAULTSORT:Zfs}}
[[Category:2005 software]]
[[Category:Disk file systems]]
[[Category:Sun Microsystems software]]
[[Category:Compression file systems]]
[[Category:RAID]]

[[ca:ZFS]]
[[cs:ZFS]]
[[de:ZFS (Dateisystem)]]
[[es:ZFS (sistema de archivos)]]
[[eu:ZFS]]
[[fr:ZFS]]
[[ko:ZFS]]
[[it:ZFS (file system)]]
[[he:ZFS]]
[[nl:ZFS]]
[[ja:ZFS]]
[[no:ZFS]]
[[pl:ZFS]]
[[pt:ZFS]]
[[ru:ZFS]]
[[simple:ZFS]]
[[sk:ZFS]]
[[sv:ZFS]]
[[th:à¸£à¸°à¸à¸à¹à¸à¹à¸¡à¹à¸à¸à¸à¸°à¹à¸à¸à¹]]
[[uk:ZFS]]
[[zh:ZFS]]</rev></revisions></page></pages></query></api>
